{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Executables \n",
    "\n",
    "**Objective:** Learn about how you can specify containers in which your job runs and executables to invoke by specifying a Transformation Catalog.\n",
    "\n",
    "In the previous notebook, we executed a simple **Hello World** workflow illustrated below, where the input data is retrieved from a remote location. \n",
    "\n",
    "\n",
    "![Hello World Workflow](../images/pipeline.svg)\n",
    "\n",
    "The python script that we are executing as part of the worklfow is a simple script. However, in real world scientific codes have complex software dependencies. Increasingly **containers** are an attractive way to package the executables and their software dependencies.  \n",
    "\n",
    "In this notebook, we will build on the previous notebook and use the Pegasus Workflow API to define a Transformation Catalog and specify the executable a job requires and the container in which it should execute. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Pegasus treats **containers** as a data dependency for the job. The container gets deployed to the node where the job runs automatically by Pegasus.\n",
    "</div>\n",
    "\n",
    "\n",
    "In this example, Pegasus will \n",
    "\n",
    "* pick up the inputs from the Pegasus website .\n",
    "* the executable invoked in a job will be run inside an application container.  \n",
    "* place the generated outputs in a directory named **output** .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python API\n",
    "\n",
    "Pegasus 5.0 introduces a new Python API, which is fully documented in the [Pegasus reference guide](https://pegasus.isi.edu/documentation/reference-guide/api-reference.html). \n",
    "\n",
    "We will mainly use the following main classes in this example\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "from Pegasus.api.replica_catalog import File\n",
    "from Pegasus.api.workflow import Job, Workflow\n",
    "from Pegasus.client._client import PegasusClientError\n",
    "from Pegasus.api.replica_catalog import File, ReplicaCatalog\n",
    "from Pegasus.api.transformation_catalog import (\n",
    "    Container,\n",
    "    Transformation,\n",
    "    TransformationCatalog,\n",
    "    TransformationSite,\n",
    ")\n",
    "```\n",
    "\n",
    "The `TransformationCatalog` object is used to specify containers and executables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# we specify directories for inputs, executables and outputs\n",
    "# - directory where the executables that the workflow uses are placed.\n",
    "# - directory where the outputs should be placed.\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "EXECUTABLES_DIR = Path(BASE_DIR / \"..\" /  \"executables\").resolve()\n",
    "OUTPUT_DIR = Path(BASE_DIR /  \"output\").resolve() \n",
    "\n",
    "# --- Replicas -----------------------------------------------------------------\n",
    "fin = File(\"f.in\").add_metadata(creator=\"vahi\")\n",
    "rc = ReplicaCatalog()\\\n",
    "    .add_replica(\"remote\", fin, \"http://download.pegasus.isi.edu/tutorial/inputs/f.in\")\\\n",
    "    .write() # written to ./replicas.yml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Transformation Catalog (Specify Executables)\n",
    "\n",
    "The Transformation Catalog serves as a repository of metadata that describes the transformations or executables used within a workflow. Each transformation represents a computational task, such as a container, script, or binary, that will be executed as part of the workflow. The catalog provides essential information about these transformations, including their unique names, versions, and the locations where they are installed or can be accessed. Additionally, the Transformation Catalog can include details about how transformations should be staged, for example, whether they should be transferred to the execution site or executed in place.\n",
    "\n",
    "In this notebook, we have are specifying a base container containing the LLM model and code(used in later notebooks). The container is hosted on Open Storage Network (OSN), and is a good example how Pegasus can transfer data as part of the workflow. The container is pulled down with a data transfer job in the workflow.\n",
    "\n",
    "The code we want to run is defined as a `Transformation()`, referencing the container. We also set profiles to specify our resource requirements (1 CPU core, 1 GPU, 10 GB RAM, and 15 GB disk), as well what type GPU we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TransformationCatalog()\n",
    "        \n",
    "wf_container = Container(\"wf_container\",\n",
    "    container_type = Container.SINGULARITY,\n",
    "    image = \"http://download.pegasus.isi.edu/containers/llm-rag/llm-rag-v2.sif\",\n",
    "    image_site = \"web\"\n",
    ")\n",
    "\n",
    "# For each type of job in the workflow specify a transformation\n",
    "# When you instantiate a Job() object, you specify a transformation name\n",
    "# which is a logical identifier for the executable you want to run\n",
    "# when the job is launched on a remote node.\n",
    "#\n",
    "# In this workflow, we have two transformations \"hello\" and \"world\",\n",
    "# with each mapping to a different executable. Can map to the same one also.\n",
    "# Note: how cpu and other resources are requested\n",
    "hello = Transformation(\"hello\", \n",
    "                         site=\"local\", \n",
    "                         pfn=\"{}/pegasus-keg.py\".format(EXECUTABLES_DIR), \n",
    "                         is_stageable=True, \n",
    "                         container=wf_container)\\\n",
    "          .add_pegasus_profiles(cores=1, memory=\"1 GB\", diskspace=\"15 GB\")\n",
    "\n",
    "world = Transformation(\"world\", \n",
    "                         site=\"local\", \n",
    "                         pfn=\"{}/pegasus-keg.py\".format(EXECUTABLES_DIR), \n",
    "                         is_stageable=True, \n",
    "                         container=wf_container)\\\n",
    "          .add_pegasus_profiles(cores=1, memory=\"1 GB\", diskspace=\"15 GB\")\n",
    "\n",
    "tc.add_containers(wf_container)\n",
    "tc.add_transformations(hello, world)\n",
    "tc.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define and Execute the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the execution site where you job to run.\n",
    "# local means the jobs run on ACCESS Pegasus itself.\n",
    "# condorpool means jobs will run on a node provisioned from an ACCESS site such as jetstream\n",
    "EXEC_SITE=\"condorpool\"\n",
    "\n",
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"hello-world\")\n",
    "\n",
    "\n",
    "finter = File(\"f.inter\")\n",
    "fout = File(\"f.out\")\n",
    "\n",
    "job_hello = Job(\"hello\")\\\n",
    "                    .add_args(\"-T\", \"3\", \"-i\", fin, \"-o {}\".format(finter))\\\n",
    "                    .add_inputs(fin)\\\n",
    "                    .add_outputs(finter)\n",
    "\n",
    "job_world = Job(\"world\")\\\n",
    "                    .add_args(\"-T\", \"3\", \"-i\", finter, \"-o {}\".format(fout))\\\n",
    "                    .add_inputs(finter)\\\n",
    "                    .add_outputs(fout)\n",
    "\n",
    "wf.add_jobs(job_hello, job_world)    \n",
    "\n",
    "# --- Run the Workflow ---------------------------------------------------\n",
    "# we have omitted the transformations_dir argument as we have specified a Transformation \n",
    "# Catalog to specify locations of executables .  \n",
    "try:\n",
    "    wf.write()\n",
    "    wf.plan(sites=[EXEC_SITE], output_dir=OUTPUT_DIR, submit=True)\\\n",
    "      .wait()      \n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting the generated output of the workflow\n",
    "\n",
    "Now lets review the output of the workflow to check where the jobs ran. The displayed hostname will have a prefix of  testpool-cpu-* . The test pool is made of nodes provisioned from the Indiana University's **Jetstream2** which is a Cloud ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/f.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
