{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Tools\n",
    "\n",
    "As mentioned before, running Pegasus is in a Jupyter notebook is very convenient for tutorials and for smaller workflows, but production workflows are most commonly submitted on dedicated HTCondor submit nodes using command line tools. This section of the tutorial uses the same workflow as we have seen in the previous sections, generated inside the notebook. Planning, submitting and checking status will be done using the command line tools.\n",
    "\n",
    "First, execute the following cell to generate the workflow. Note that we are just writing it out at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "\n",
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"                                                                    \n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "\n",
    "# Allow the jobs to run on the test cluster. You do not need to provision\n",
    "# resources from your own allocations in this case, but the cluster is small\n",
    "# and should not be used for production workloads.\n",
    "props.add_site_profile(\"condorpool\", \"condor\", \"+run_on_test_cluster\", \"true\")\n",
    "\n",
    "props.write() # written to ./pegasus.properties \n",
    "\n",
    "# --- Replicas -----------------------------------------------------------------\n",
    "with open(\"f.a\", \"w\") as f:\n",
    "   f.write(\"This is sample input to KEG\")\n",
    "\n",
    "fa = File(\"f.a\").add_metadata(creator=\"ryan\")\n",
    "rc = ReplicaCatalog().add_replica(\"local\", fa, Path(\".\").resolve() / \"f.a\")\n",
    "\n",
    "# --- Container ----------------------------------------------------------\n",
    "\n",
    "base_container = Container(\n",
    "                  \"base-container\",\n",
    "                  Container.SINGULARITY,\n",
    "                  image=\"docker://karanvahi/pegasus-tutorial-minimal\"\n",
    "    \n",
    "                  # comment out the location below (and comment the above location) \n",
    "                  # if you run into docker rate pull limits. Do this if your  \n",
    "                  # workflow fails on the first try with stage-in jobs fail \n",
    "                  # with error like ERROR: toomanyrequests: Too Many Requests. OR\n",
    "                  # You have reached your pull rate limit. You may increase \n",
    "                  # the limit by authenticating and upgrading: \n",
    "                  # ttps://www.docker.com/increase-rate-limits. \n",
    "                  # You must authenticate your pull requests.\n",
    "                  #\n",
    "                  # This is why Pegasus supports tar files of containers, \n",
    "                  # and also ensures the pull from a docker hub happens only \n",
    "                  # once per workflow\n",
    "    \n",
    "                  #image=\"http://download.pegasus.isi.edu/pegasus/tutorial/pegasus-tutorial-minimal.tar.gz\"\n",
    "               )\n",
    "\n",
    "\n",
    "# --- Transformations ----------------------------------------------------------\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "findrange = Transformation(\n",
    "                \"findrange\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "analyze = Transformation(\n",
    "                \"analyze\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=True,\n",
    "                container=base_container,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            ).add_profiles(Namespace.CONDOR, request_disk=\"120MB\")\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_containers(base_container)\\\n",
    "    .add_transformations(preprocess, findrange, analyze)\\\n",
    "    .write() # written to ./transformations.yml\n",
    "\n",
    "# --- Sites -----------------------------------------------------------------\n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "job_env_file = Path(str(BASE_DIR) + \"/../tools/job-env-setup.sh\").resolve()\n",
    "local.add_pegasus_profile(pegasus_lite_env_source=job_env_file)\n",
    "\n",
    "sc = SiteCatalog()\\\n",
    "   .add_sites(local)\\\n",
    "   .write() # written to ./sites.yml\n",
    "\n",
    "# --- Workflow -----------------------------------------------------------------\n",
    "'''\n",
    "                     [f.b1] - (findrange) - [f.c1]\n",
    "                     /                             \\\n",
    "[f.a] - (preprocess)                               (analyze) - [f.d]\n",
    "                     \\                             /\n",
    "                     [f.b2] - (findrange) - [f.c2]\n",
    "\n",
    "'''\n",
    "wf = Workflow(\"blackdiamond\")\n",
    "\n",
    "fb1 = File(\"f.b1\")\n",
    "fb2 = File(\"f.b2\")\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                     .add_args(\"-a\", \"preprocess\", \"-T\", \"3\", \"-i\", fa, \"-o\", fb1, fb2)\\\n",
    "                     .add_inputs(fa)\\\n",
    "                     .add_outputs(fb1, fb2)\n",
    "\n",
    "fc1 = File(\"f.c1\")\n",
    "job_findrange_1 = Job(findrange)\\\n",
    "                     .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb1, \"-o\", fc1)\\\n",
    "                     .add_inputs(fb1)\\\n",
    "                     .add_outputs(fc1)\n",
    "\n",
    "fc2 = File(\"f.c2\")\n",
    "job_findrange_2 = Job(findrange)\\\n",
    "                     .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb2, \"-o\", fc2)\\\n",
    "                     .add_inputs(fb2)\\\n",
    "                     .add_outputs(fc2)\n",
    "\n",
    "fd = File(\"f.d\")\n",
    "job_analyze = Job(analyze)\\\n",
    "               .add_args(\"-a\", \"analyze\", \"-T\", \"3\", \"-i\", fc1, fc2, \"-o\", fd)\\\n",
    "               .add_inputs(fc1, fc2)\\\n",
    "               .add_outputs(fd)\n",
    "\n",
    "wf.add_jobs(job_preprocess, job_findrange_1, job_findrange_2, job_analyze)\n",
    "wf.add_replica_catalog(rc)\n",
    "\n",
    "\n",
    "wf.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Opening the Jupyter terminal\n",
    "\n",
    "To open a new terminal window, navigate back to the listings tab of Jupyter notebook. This is where you have been opening all the sections from. In the top right corner of the listing, click `New` and then `Terminal`. It looks something like:\n",
    "\n",
    "![Terminal Start](../images/terminal-start.png)\n",
    "\n",
    "Once started, arrange your browser tabs/windows side by side so that you can see these instructions and the terminal window at the same time. In the following sections, when you are presented with a `$`, that means it is a command you can type in or copy and paste into the terminal window. Sometimes you have to substitute your own values and that is highlighted with square brackets `[]`.\n",
    "\n",
    "First, cd to the correct directory:\n",
    "\n",
    "    $ cd ~/ACCESS-Pegasus-Examples/03-Command-Line-Tools/\n",
    "    \n",
    "If you run `ls`, you should see these files:\n",
    "\n",
    "    $ ls\n",
    "    03-Command-Line-Tools.ipynb\n",
    "    f.a\n",
    "    pegasus.properties\n",
    "    workflow.yml\n",
    "    \n",
    "The 3 latter ones were just generated by the cell above.\n",
    "\n",
    "## 2. Planning and submitting\n",
    "\n",
    "We can now plan and submit the workflow by running:\n",
    "\n",
    "    $ pegasus-plan --submit workflow.yml\n",
    "    \n",
    "In the output of the plan command, you will see a reference to several other Pegasus commands such as pegasus-status. More importantly, a workflow directory was generated for the new workflow instance. This directory is the handle to the workflow instance and used by Pegasus command line tools. Some useful tools to know about:\n",
    "\n",
    " * **pegasus-status -v [wfdir]** Provides status on a workflow instance\n",
    " * **pegasus-analyzer [wfdir]** Provides debugging clues why a workflow failed. Run this after a workflow has failed\n",
    " * **pegasus-statistics [wfdir]** Provides statistics, such as walltimes, on a workflow after it has completed\n",
    " * **pegasus-remove [wfdir]** Removes a workflow from the system\n",
    "\n",
    "\n",
    "## 3. Workflow status\n",
    "\n",
    "Use the workflow directory given in the output of the `pegasus-plan` command to determine the status of your workflow:\n",
    "\n",
    "    $ pegasus-status -v [wfdir]\n",
    "\n",
    "The flags `-l` and `-v` are just two different version of more verbose output. Please see `pegasus-status --help` to see all the options available.\n",
    "\n",
    "You can keep running `pegasus-status` until the workflow has completed, or you can use the `-w` flag to mimic the `wait()` function we used in the API. This flag will make `pegasus-status` run periodically until the workflow is complete:\n",
    "\n",
    "    $ pegasus-status -v -w [wfdir]\n",
    "    \n",
    "\n",
    "## 4. Workflow statistics\n",
    "\n",
    "Once the workflow is complete, you can extract statistics from the provenance database:\n",
    "\n",
    "    $ pegasus-statistics -s all [wfdir]\n",
    "\n",
    " \n",
    "## What's Next?\n",
    "\n",
    "The next notebook is `04-Summary/` that summarizes what we have learnt so far. Next you are welcome to try real world workflow examples from domains such as Artificial-Intelligence, Astronomy and Bioinformatics. You can go into those folders, to access the notebooks and try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
