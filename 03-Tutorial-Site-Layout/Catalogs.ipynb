{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80870775",
   "metadata": {},
   "source": [
    "# Site Catalog\n",
    "\n",
    "**Objective:** Learn about how Pegasus uses a Site Catalog to learn information about the layout of the resource you want to execute your workflow on.\n",
    "\n",
    "\n",
    "The Site Catalog defines the computational environments where the workflow's tasks will execute. Each \"site\" in the catalog represents a distinct resource, such as a local machine, high-performance computing cluster, or cloud platform. The catalog provides detailed information about the resources at each site, including \n",
    "\n",
    "- paths for shared and local storage, ensuring efficient data management by defining where input, output, and intermediate data will be stored \n",
    "- file server endpoints defining how data will be staged in and out of the site.\n",
    "- any profiles (such as environment variables, condor attributes) that need to be applied to each job that executes on that resource.\n",
    "- any default job resource requirements such as how much memory a job running on a node requires.\n",
    "\n",
    "By default, Pegasus assumes two sites (which is why you have not seen it yet in the notebooks)\n",
    "\n",
    "- *local* : The _local_ site refers to the submit host which is literally the machine on which Pegasus is installed and you are running the notebook or any of the pegasus command line tools such as `pegasus-plan`, `pegasus-status` etc. \n",
    "\n",
    "- *condorpool* : The _condorpool_ site refers to the HTCondor pool we want to run our jobs in. There is no need to specify directories in this case, as the work directory is automatically assigned by HTCondor.\n",
    "\n",
    "In the ACCESS Pegasus setup, the _condorpool_ site is made of a TestPool provisioned automatically from ACCESS sites such as *Jetstream2* .\n",
    "\n",
    "However, you may find yourself defining a **site** when you need to\n",
    "\n",
    "- override or associate certain environment variables that should apply to your jobs\n",
    "- execute your worklfow on a remote HPC cluster such as SDSC expanse where you need to use the shared filesystem.\n",
    "\n",
    "The code example below shows how you can define a Site Catalog and override the **condorpool** site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog(self):\n",
    "        self.sc = SiteCatalog()\n",
    "        condorpool = (Site(\"condorpool\")\n",
    "                        .add_condor_profile(universe=\"container\")\n",
    "                        .add_pegasus_profile(\n",
    "                            style=\"condor\"\n",
    "                        )\n",
    "                    )\n",
    "        condorpool.add_profiles(Namespace.ENV, LANG='C')\n",
    "        condorpool.add_profiles(Namespace.ENV, PYTHONUNBUFFERED='1')\n",
    "        \n",
    "        # exclude the ACCESS Pegasus TestPool \n",
    "        #condorpool.add_condor_profile(requirements=\"TestPool =!= True\")\n",
    "\n",
    "        # If you want to run on OSG, please specify your OSG ProjectName. For testing, feel\n",
    "        # free to use the USC_Deelman project (the PI of the Pegasus project).For\n",
    "        # production work, please use your own project.\n",
    "        condorpool.add_profiles(Namespace.CONDOR, key=\"+ProjectName\", value=\"\\\"USC_Deelman\\\"\")\n",
    "        \n",
    "        self.sc.add_sites(condorpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fa8f5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note how we also set profiles on the _condorpool_ site. There are both environment and HTCondor profiles specified, and they will be applied to all jobs on that site. See the [Configuration](https://pegasus.isi.edu/documentation/reference-guide/configuration.html) chapter in the Pegasus manual.\n",
    "\n",
    "### Running on a shared filesystem on a ACCESS Resource\n",
    "\n",
    "If you want to run your workflow on a particular ACCESS Resource leveraging the shared filesystem on that resource, then you need to tie-in your **allocation** for the resource, and provision nodes for your workflows using that allocation. These concepts are covered in subsequent notebooks on provisioning and running on a shared fileystem.\n",
    "\n",
    "We describe below how you would describe such as site e.g. SDSC Expanse resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e390bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog(self):\n",
    "        sc = SiteCatalog()\n",
    "        expanse = (Site(\"expanse\")\n",
    "        .add_pegasus_profile(style=\"condor\")\n",
    "        )\n",
    "        \n",
    "        # note the variables below should be updated to refer to your\n",
    "        # username on expanse starting. There are of the form uxXXXXX\n",
    "        cluster_shared_dir = \"/expanse/lustre/scratch/EXPANSE_USERNAME/temp_project\"\n",
    "        cluster_home_dir = \"/home/EXPANSE_USERNAME\"\n",
    "        \n",
    "        exec_site_shared_scratch_dir = os.path.join(cluster_shared_dir, \"pegasuswfs/scratch\")\n",
    "        exec_site_shared_storage_dir = os.path.join(cluster_home_dir, \"pegasuswfs/outputs\")\n",
    "        \n",
    "        expanse.add_directories(\n",
    "            Directory(Directory.SHARED_SCRATCH, exec_site_shared_scratch_dir)\n",
    "            .add_file_servers(FileServer(\"file://\" + exec_site_shared_scratch_dir, Operation.ALL)),\n",
    "            Directory(Directory.LOCAL_STORAGE, exec_site_shared_storage_dir)\n",
    "            .add_file_servers(FileServer(\"file://\" + exec_site_shared_storage_dir, Operation.ALL))\n",
    "        )\n",
    "        expanse.add_profiles(Namespace.ENV, LANG='C')\n",
    "        expanse.add_profiles(Namespace.ENV, PYTHONUNBUFFERED='1')\n",
    "\n",
    "        # exclude the ACCESS Pegasus TestPool\n",
    "        # we want it to run on our provisioned resources.\n",
    "        expanse.add_condor_profile(requirements=\"TestPool =!= True\")\n",
    "        \n",
    "        sc.add_sites(expanse)\n",
    "        return sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed273e",
   "metadata": {},
   "source": [
    "### Running on a local HPC cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd149e6e",
   "metadata": {},
   "source": [
    "If you are doing this training, and want to setup Pegasus to run on your local campus cluster, please refer to the [documentation](https://pegasus.isi.edu/documentation/user-guide/deployment-scenarios.html#hpc-clusters-system-install) for full details.\n",
    "\n",
    "In general, to describe a local SLURM cluster you will do the setup as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog_for_local_slurm(self):\n",
    "        sc = SiteCatalog()\n",
    "        slurm_scratch_dir = \"{}/SLURM/work\".format(BASE_DIR)\n",
    "        slurm_storage_dir = \"{}/SLURM/storage\".format(BASE_DIR)\n",
    "\n",
    "        slurm = Site(\"slurm\")\\\n",
    "            .add_directories(\n",
    "            Directory(Directory.SHARED_SCRATCH, slurm_scratch_dir)\n",
    "                .add_file_servers(FileServer(\"file://\" + slurm_scratch_dir, Operation.ALL)),\n",
    "            Directory(Directory.LOCAL_STORAGE, slurm_storage_dir)\n",
    "                .add_file_servers(FileServer(\"file://\" + slurm_storage_dir, Operation.ALL)))\n",
    "\n",
    "        slurm.add_pegasus_profile(\n",
    "                                style=\"glite\",\n",
    "                                queue=slurm_partition,\n",
    "                                project=slurm_account,\n",
    "                                data_configuration=\"nonsharedfs\",\n",
    "                                auxillary_local=\"true\",\n",
    "                                nodes=1,\n",
    "                                ppn=1,\n",
    "                                runtime=1800,\n",
    "                                clusters_num=2\n",
    "                            )\n",
    "        slurm.add_condor_profile(grid_resource=\"batch slurm\")\n",
    "        \n",
    "        sc.add_sites(slurm)\n",
    "        return sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f3c550",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By going through these notebooks in order you have now learnt about how Pegasus uses catalogs to map an abstract workflow to an executable workflow.\n",
    "\n",
    "The Abstract Workflow description that you specify to Pegasus is portable, and usually does not contain any locations to physical input files, executables or cluster end points where jobs are executed. Pegasus uses three information catalogs during the planning process.\n",
    "\n",
    "<img src=\"images/catalogs.png\"/>\n",
    "\n",
    "The Pegasus documentation provides more details about catalogs [here](https://pegasus.isi.edu/documentation/user-guide/creating-workflows.html#catalogs)\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "\n",
    "We will now progress to running a real LLM RAG workflow, that leverages all the concepts learnt so far and describes all the 3 catalogs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
