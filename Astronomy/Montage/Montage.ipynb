{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#880000\"> Montage Parallelization Using the Pegasus Workflow Manager\n",
    "    \n",
    "The purpose of this notebook is to illustrate the use of the Pegasus Workflow\n",
    "Manager (plus the Condor Job Manager) to run Montage astronomical image\n",
    "processing jobs.  Such jobs tend to have large chunks of heavily parallelizable\n",
    "processing (<i>e.g.</i>, hundreds of images that can be reprojected in parallel) and\n",
    "Pegasus/Condor supports this intrinsically.\n",
    "\n",
    "This Notebook is intended to demonstrate proof-of-concept parallelization. Condor processing \n",
    "contains some overhead and so execution of highly granular jobs that run quickly\n",
    "(some of our reprojections run in around a second) is highly inefficient.  In practice,\n",
    "our application would subdivide the processing into subsets that each take\n",
    "5-10 minutes and treat each of these subsets as a separate Condor job. This\n",
    "is easy for Montage, which already has tools for running a set of the small\n",
    "jobs as one big job.\n",
    "    \n",
    "\n",
    "#### LOCATION, MOSAIC SIZE AND DATA SET\n",
    "\n",
    "<img src=\"images/M17_location.png\" style=\"float:right\"/>\n",
    "\n",
    "For our Montage processing, we need to know what location on the sky to mosaic, how big to make the image (in degrees), and which dataset to use. The location here is given as an object name but coordinates work just as well. We will create a mosaic of M17 (the Omega Nebula, 1 degree x 1 degree, in the 2MASS J-band):\n",
    "\n",
    "From Wikipedia (https://en.wikipedia.org/wiki/Omega_Nebula)\n",
    "\n",
    "<i>The Omega Nebula (M17) is between 5,000 and 6,000 light-years from Earth and it spans some 15 light-years in diameter. The cloud of interstellar matter of which this nebula is a part is roughly 40 light-years in diameter and has a mass of 30,000 solar masses. The total mass of the Omega Nebula is an estimated 800 solar masses.\n",
    "\n",
    "It is considered one of the brightest and most massive star-forming regions of our galaxy. Its local geometry is similar to the Orion Nebula except that it is viewed edge-on rather than face-on.\n",
    "\n",
    "The open cluster NGC 6618 lies embedded in the nebulosity and causes the gases of the nebula to shine due to radiation from these hot, young stars; however, the actual number of stars in the nebula is much higher - up to 800, 100 of spectral type earlier than B9, and 9 of spectral type O, plus over a thousand stars in formation on its outer regions. It is also one of the youngest clusters known, with an age of just 1 million years.</i>\n",
    "    \n",
    "#### MONTAGE PROCESSING MODEL\n",
    "\n",
    "There is a standard flow to the processing of a collection of images to make a mosaic.  If the images all have the same background level, then you simply reprojected them all to a common frame and coadd the projected images.\n",
    "  \n",
    "If the backgrounds vary, as is usually the case, it gets more complicated.  The  Montage approach to global background matching is to measure the differences between every pair of overlapping images individually (by actually taking the image difference and then fitting it), then model a set of corrections that minimize the cumulative difference measure.\n",
    "  \n",
    "Normally, we perform tasks like determining the overlaps when we get to the point of needing them.  This has the advantage of not specifying an overlap  where the an image actually failed to reproject (usually because it turns  out to have no data in our region of interest).  But this will not cause an error in the Montage processing and waiting complicates the process of setting up a Pegasus workflow, as we would have to run a set of small workflows rather than one big one.\n",
    "  \n",
    "So here we will assume that all images get processed through all the steps above and we will generate a list of overlaps based on the set of input images (determined before building the workflow).\n",
    "\n",
    "This results in  using Montage in two contexts and in slightly different  ways.  Up front we use a few Montage modules through their Python bindings to gather the information we need to create the workflow.  Then later, when  the workflow is running it uses the C binding (stand-alone processes as enumerating in the Transformations list above) to do the compute-intensive image processing.  Running the workflow occurs outside Python and potentially  much later and on different machines.\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center><a href=\"images/graph.png\" target=\"_blank\"><img src=\"images/graph.png\" style=\"width:800px; height:600px;\" /></a></center>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "This graph is for a simpler mosaic than we have.  Our region has 49 images and over 200 diffs and the graph is way too wide to view properly.  Here there are only six images and nine overlaps.\n",
    "    \n",
    "<br>The descriptions for various jobs in the worklfow are listed in a table below\n",
    "\n",
    "| Job Label    | Description                                                    |\n",
    "| -------------|----------------------------------------------------------------|\n",
    "| mProjectQL   | loop over the archive image list to define the reprojection jobs|\n",
    "| mDiffFit     | loop over the differences list to generate the background difference fits|\n",
    "| mConcatFit   | gather all the background fit return structures into a single table|\n",
    "| mBgModel     | model the background correction parameters|\n",
    "| mBackground  | subtract the correction plane for each from the reprojected images |\n",
    "| mAdd         | coadd reprojected and corrected together into a single mosaic image|\n",
    "| mShrink      | make a shrunken version of the file|\n",
    "| mViewer      | stretch it to enhance the full dynamic range, make 1500 pixels image|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Gather input data\n",
    "\n",
    "The starting point for our processing is a set of images covering a region on the sky and a specification for exactly what projection (coordinate system, map projection, pixel scale) we want.  \n",
    "\n",
    "<b>Note:</b> In addition to the original list of image we retrieve remotely,  it is convenient to have lists of the intermediate image sets (projected, background-corrected).  While we can't can't be sure all the images in our original list will survive to this point (e.g., some may not have real data that overlaps with our region of interest), the processing is robust against this.  So we will use the input list to predict the other lists.\n",
    "We have a utility for this but it has not yet been converted to library form for use from Python. So for now we run this step as a child process using os.system()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "from astropy.io import ascii\n",
    "\n",
    "from MontagePy.main import *\n",
    "from Pegasus.api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the data to mosaic\n",
    "\n",
    "location  = 'M 17'\n",
    "size      =  1.0\n",
    "dataset   = '2MASS'\n",
    "band      = 'J'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mHdr(location, size, size, 'region.hdr')\n",
    "\n",
    "rtn = mArchiveList(dataset, band, location, size, size, 'remote_big.tbl')\n",
    "\n",
    "print(rtn)\n",
    "\n",
    "rtn = mCoverageCheck('remote_big.tbl', 'remote.tbl', hdrfile='region.hdr', mode=4, array=[])\n",
    "\n",
    "print(rtn)\n",
    "\n",
    "os.system('/opt/montage/default/bin/mDAGTbls remote.tbl region.hdr rimages.tbl pimages.tbl cimages.tbl')\n",
    "\n",
    "rtn = mOverlaps('rimages.tbl', 'diffs.tbl')\n",
    "\n",
    "print(rtn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get to processing the difference fitting, we will also need a list of the fits to support the collecting the data into a single table.  This is just a variant of the diffs.tbl info and we can generate it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitsfile = open('fitslist.tbl', 'w+')\n",
    "\n",
    "line = '|{:^10}|{:^10}|{:^24}|\\n'.format('cntr1', 'cntr2', 'stat')\n",
    "\n",
    "fitsfile.write(line)\n",
    "\n",
    "diffdata = ascii.read('diffs.tbl', format='ipac')\n",
    "\n",
    "for rec in diffdata:\n",
    "\n",
    "    rtnfile = rec['diff']\n",
    "    rtnfile = rtnfile.replace('.fits', '.txt')\n",
    "\n",
    "    line = ' {:>10} {:>10} {:<24} \\n'.format(rec['cntr1'], rec['cntr2'], rtnfile)\n",
    "\n",
    "    fitsfile.write(line)\n",
    "\n",
    "fitsfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Building the workflow\n",
    "\n",
    "<img src=\"images/M17_coverage.png\" style=\"float:left; padding:30px;\" /> \n",
    "\n",
    "For the workflow, we define all the individual processing jobs, what inputs they depend on (either from the Replica Catalog or files we created along the way) and what outputs they generate.  Pegasus will infer the shape of the \"graph\" it constructs and the actual set of jobs for systems like HTCondor from this information.\n",
    "\n",
    "<b>Note:</b>  The Replica Catalog is so named because in a large system it can be allowed to draw from a set of duplicate instances, choosing whichever is going to be the most efficient.\n",
    "\n",
    "There are three sets of things we need to keep track of\n",
    "(Pegasus stores this information in SQLite database files).\n",
    "\n",
    "The \"<b>replica catalog</b>\": All the input files for the processing and how to access them.  Here the data is accessed via URL download,  but local files and some specialize access tools can also be used.\n",
    "\n",
    "The \"<b>transformations</b>\" are the processing tools (here our Montage modules) and where to find them.  We will have them all available on the same machine but sometimes they too are downloaded from somewhere else.\n",
    "\n",
    "Finally, the \"<b>workflow</b>\" enumerates the specific steps in the processing.  We lay out the jobs we want run (transforms, argument lists, which files are input and which are output, <i>etc.</i>)  Pegasus deduces when things can be run in large part by when the precursor files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on basic logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# Set up a set of 'properties' for the Pegasus processing\n",
    "\n",
    "props = Properties()\n",
    "\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"\n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.mode\"] = \"development\"\n",
    "\n",
    "# Allow the jobs to run on the test cluster. You do not need to provision\n",
    "# resources from your own allocations in this case, but the cluster is small\n",
    "# and should not be used for production workloads.\n",
    "# props.add_site_profile(\"condorpool\", \"condor\", \"+run_on_test_cluster\", \"true\")\n",
    "\n",
    "props.write()\n",
    "\n",
    "rc = ReplicaCatalog()\n",
    "tc = TransformationCatalog()\n",
    "wf = Workflow(\"Montage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations \n",
    "Below all the Montage modules the processing will use are set up as \"transformations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranformations\n",
    "bin = '/opt/Montage/bin/'\n",
    "container = Container('montage',\n",
    "            Container.SINGULARITY,\n",
    "            'https://data.isi.edu/montage/images/montage-adass.sif'\n",
    "            ).add_env(MONTAGE_HOME='/opt/Montage')\n",
    "tc.add_containers(container)\n",
    "\n",
    "project        = Transformation(\n",
    "                 \"mProjectQL\",\n",
    "                 pfn=bin + \"mProjectQL\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "dif           = Transformation(\n",
    "                 \"mDiff\",\n",
    "                 pfn=bin + \"mDiff\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "fitplane       = Transformation(\n",
    "                 \"mFitplane\",\n",
    "                 pfn=bin + \"mFitplane\",\n",
    "                 site=\"condorpool\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "diff_fit       = Transformation(\n",
    "                 \"mDiffFit\",\n",
    "                 pfn=bin + \"mDiffFit\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False).add_requirement(fitplane).add_requirement(dif).add_env(PATH=\"/usr/bin:/bin:.\")\n",
    "\n",
    "concat_fits    = Transformation(\n",
    "                 \"mConcatFit\",\n",
    "                 pfn=bin + \"mConcatFit\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "bg_model       = Transformation(\n",
    "                 \"mBgModel\",\n",
    "                 pfn=bin + \"mBgModel\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "bg_model.add_profiles(Namespace.CONDOR, key='request_memory', value='256 MB')\n",
    "\n",
    "background     = Transformation(\n",
    "                 \"mBackground\",\n",
    "                 pfn=bin + \"mBackground\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "add            = Transformation(\n",
    "                 \"mAddMem\",\n",
    "                 pfn=bin + \"mAddMem\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "add.add_profiles(Namespace.CONDOR, key='request_memory', value='512 MB')\n",
    "\n",
    "shrink         = Transformation(\n",
    "                 \"mShrink\",\n",
    "                 pfn=bin + \"mShrink\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "viewer         = Transformation(\n",
    "                 \"mViewer\",\n",
    "                 pfn=bin + \"mViewer\",\n",
    "                 site=\"local\",\n",
    "                 container=container,\n",
    "                 is_stageable=False)\n",
    "\n",
    "tc.add_transformations(project, diff_fit, concat_fits, dif, fitplane)\n",
    "tc.add_transformations(bg_model, background, add, shrink, viewer)\n",
    "\n",
    "tc.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some of these files in the replica table so we can find them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdrfile = File('region.hdr')\n",
    "pimages = File('pimages.tbl')\n",
    "cimages = File('cimages.tbl')\n",
    "flist   = File('fitslist.tbl')\n",
    "\n",
    "site = 'local'\n",
    "\n",
    "rc.add_replica(site, hdrfile, Path(\".\").resolve() / \"region.hdr\")\n",
    "rc.add_replica(site, pimages, Path(\".\").resolve() / \"pimages.tbl\")\n",
    "rc.add_replica(site, cimages, Path(\".\").resolve() / \"cimages.tbl\")\n",
    "rc.add_replica(site, flist,   Path(\".\").resolve() / \"fitslist.tbl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image reprojection - mProjectQL\n",
    "\n",
    "Loop over the archive image list to define the reprojection jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### CREATE JOBS ###########################################################\n",
    "remotedata = ascii.read('remote.tbl', format='ipac')\n",
    "\n",
    "for rec in remotedata:\n",
    "\n",
    "    rawimg = File(rec['file'])\n",
    "\n",
    "    rawpfn = rec['URL']\n",
    "\n",
    "    rc.add_replica(site, rawimg, rawpfn)\n",
    "\n",
    "    projimg = File('p' + rec['file'])\n",
    "\n",
    "    job = Job(project)\\\n",
    "            .add_args(rawimg, projimg, hdrfile)\\\n",
    "            .add_inputs(rawimg)\\\n",
    "            .add_inputs(hdrfile)\\\n",
    "            .add_outputs(projimg)\n",
    "\n",
    "    wf.add_jobs(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Overlap.png\" style=\"float:left; padding:0px 20px 0px 0px;\" /> \n",
    "\n",
    "#### Difference Fitting  - mDiffFit\n",
    "\n",
    "Loop over the differences list to generate the background difference fits. The figure on the left shows two overlapping images flanking their difference.  Slight positional offsets are accentuated around bright sources but do not affect the difference fitting as it rejects such positive/negative excursions.\n",
    "\n",
    "Given the way Pegasus (and HTCondor) operate, there is really no way to convey information from one job to another other than to write it into files.  Montage modules usually communicate by sending a completion message to stdout but luckily also have a mode where this same message is written to a file, facilitating Pegasus use (and use by other similar systems).  This is paired with a Montage module that  can collect up all this information and construct a data file from it (here a table of the pairwise difference fit parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffdata = ascii.read('diffs.tbl', format='ipac')\n",
    "\n",
    "for rec in diffdata:\n",
    "    plusimg  = File('p' + rec['plus'])\n",
    "    minusimg = File('p' + rec['minus'])\n",
    "\n",
    "    diffimg  = File(rec['diff'])\n",
    "\n",
    "    rtnfile = rec['diff']\n",
    "    rtnfile = rtnfile.replace('.fits', '.txt')\n",
    "\n",
    "    diffrtn = File(rtnfile)\n",
    "\n",
    "    job = Job(diff_fit)\\\n",
    "            .add_args('-n', '-s', diffrtn, plusimg, minusimg, diffimg, hdrfile)\\\n",
    "            .add_inputs(plusimg)\\\n",
    "            .add_inputs(minusimg)\\\n",
    "            .add_inputs(hdrfile)\\\n",
    "            .add_outputs(diffimg)\\\n",
    "            .add_outputs(diffrtn)\n",
    "\n",
    "    wf.add_jobs(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather background return structured - mConcatFits\n",
    "\n",
    "Now we have to gather all the background fit return structures into a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = File('fits.tbl')\n",
    "\n",
    "job = Job(concat_fits)\\\n",
    "        .add_args(flist, fits, '.')\\\n",
    "        .add_inputs(flist)\\\n",
    "        .add_outputs(fits)\n",
    "\n",
    "for rec in diffdata:\n",
    "\n",
    "    rtnfile = rec['diff']\n",
    "    rtnfile = rtnfile.replace('.fits', '.txt')\n",
    "\n",
    "    diffrtn = File(rtnfile)\n",
    "\n",
    "    job.add_inputs(diffrtn)\n",
    "\n",
    "wf.add_jobs(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model the backgound correction factors - mBgModel\n",
    "\n",
    "Given the list of background fits and the original image list, we can  model the background correction parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = File('corrections.tbl')\n",
    "\n",
    "job = Job(bg_model)\\\n",
    "        .add_args('-t', '-a', pimages, fits, corrections)\\\n",
    "        .add_inputs(pimages)\\\n",
    "        .add_inputs(fits)\\\n",
    "        .add_outputs(corrections)\n",
    "\n",
    "wf.add_jobs(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectify reprojected images - mBackground\n",
    "\n",
    "Looping over the images yet again, we subtract the correction plane for each from the reprojected images.  Note: we still have the 'remotedata' structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rec in remotedata:\n",
    "\n",
    "    projimg = File('p' + rec['file'])\n",
    "    corrimg = File('c' + rec['file'])\n",
    "\n",
    "    job = Job(background)\\\n",
    "            .add_args('-n', '-t', projimg, corrimg, pimages, corrections)\\\n",
    "            .add_inputs(projimg)\\\n",
    "            .add_inputs(pimages)\\\n",
    "            .add_inputs(corrections)\\\n",
    "            .add_outputs(corrimg)\n",
    "\n",
    "    wf.add_jobs(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-add corrected rectified images - mAdd\n",
    "\n",
    "Finally, we have a set of reprojected (co-registered), background- corrected images and can coadd them together into a single mosaic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic  = File('mosaic.fits')\n",
    "\n",
    "job = Job(add)\\\n",
    "        .add_args('-n', cimages, hdrfile, mosaic)\\\n",
    "        .add_inputs(cimages)\\\n",
    "        .add_inputs(hdrfile)\\\n",
    "        .add_outputs(mosaic)\n",
    "\n",
    "for rec in remotedata:\n",
    "\n",
    "    corrimg = File('c' + rec['file'])\n",
    "    \n",
    "    job.add_inputs(corrimg);\n",
    "\n",
    "wf.add_jobs(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrink the mosaic for visualization - mShrink\n",
    "\n",
    "The FITS mosaic is the real product of this workflow but we usually make a PNG of it as the fasted way to validate the processing.  The mosaic is often way too large for proper viewing, so we will first make a shrunken version of the file (this works equally well if the  image is too small for proper viewing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrunken = File('shrunken.fits')\n",
    "\n",
    "job = Job(shrink)\\\n",
    "        .add_args('-f', mosaic, shrunken, '1500')\\\n",
    "        .add_inputs(mosaic)\\\n",
    "        .add_outputs(shrunken)\n",
    "\n",
    "wf.add_jobs(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a PNG of the shrunken image - mViewer\n",
    "\n",
    "The purpose of this PNG is mostly debugging, so we will stretch it to enhance the full dynamic range and make it 1500 pixels across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png = File('mosaic.png')\n",
    "\n",
    "job = Job(viewer)\\\n",
    "        .add_args('-ct', '1', '-gray', shrunken,\\\n",
    "                  'min', 'max', 'gaussian-log', '-out', png)\\\n",
    "        .add_inputs(shrunken)\\\n",
    "        .add_outputs(png)\n",
    "\n",
    "wf.add_jobs(job)\n",
    "\n",
    "rc.write()\n",
    "wf.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Plan and Submit the workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site **condorpool** i.e the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('\\nGraphing the workflow ...')\n",
    "\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.pdf\")\n",
    "\n",
    "    try:\n",
    "        print('\\nGraph succeeded. Making the plan ...')\n",
    "\n",
    "        wf.plan(submit=True).wait()\n",
    "\n",
    "        try:\n",
    "            print('\\nPlan finished. Generating statistics ...')\n",
    "\n",
    "            wf.statistics()\n",
    "\n",
    "        except PegasusClientError as e:\n",
    "\n",
    "            print('\\nStatistics failed:')\n",
    "            print(e)\n",
    "\n",
    "    except PegasusClientError as e:\n",
    "\n",
    "        try:\n",
    "            print('\\nPlan failed:')\n",
    "            print(e)\n",
    "\n",
    "            print('\\nAnalyzing:')\n",
    "            wf.analyze()\n",
    "\n",
    "\n",
    "        except PegasusClientError as e:\n",
    "\n",
    "            print('\\nAnalysis failed:')\n",
    "            print(e)\n",
    "\n",
    "except PegasusClientError as e:\n",
    "\n",
    "    print('\\nGraph failed:')\n",
    "    print(e)\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Launch Pilots Jobs on ACCESS resources\n",
    "\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "A pilot can run multiple user jobs - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    "\n",
    "A pilot is partitionable - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    "\n",
    "A pilot will only run jobs for the user who started it.\n",
    "\n",
    "The process of starting pilots is described in the [ACCESS Pegasus Documentation](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html)\n",
    "\n",
    "## 5. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "<br>`3-Band Color :` We can extend the workflow (our add two more workflows) to generate mosaics for the other two 2MASS wavelengths and combine them into a color mosaic.  Since at infrared wavelengths we can see into (and partially through) the dust cloud associated with M17, the relationship between the cloud and the star-forming region becomes much clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/M17.png\" style=\"width: 350px;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
