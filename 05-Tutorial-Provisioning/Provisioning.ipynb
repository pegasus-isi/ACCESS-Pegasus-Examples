{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b691db",
   "metadata": {},
   "source": [
    "# Provisioning\n",
    "\n",
    "ACCESS Pegasus offers flexible provisioning options to accommodate various computational needs across High-Performance Computing (HPC), High-Throughput Computing (HTC), and cloud environments. Provsioning is the process of \n",
    "bringing in additional compute nodes into an overlay pool.\n",
    "\n",
    "A virtual pool overlay refers to the creation of a unified and cohesive execution environment that spans multiple distributed resources. In ACCESS Pegasus, this is achieved by overlaying an HTCondor pool across one or more ACCESS resource providers. ACCESS Pegasus uses HTCondor for the overlay pool.\n",
    "\n",
    "Pilot jobs (also known as glidein jobs) are placeholder jobs submitted to computing resources, which, upon execution, transform into dynamic execution environments capable of running multiple user tasks. This approach allows for efficient resource utilization and reduces the overhead associated with scheduling individual tasks.\n",
    "\n",
    "Where to pull these resources from, depends mostly on where you have an allocation. The options are:\n",
    "\n",
    " 1. TestPool\n",
    " 2. Cloud\n",
    " 3. HTCondor Annex\n",
    " 4. OSPool\n",
    "\n",
    "<img src=\"images/ACCESS-Pegasus-Pools.png\"/>\n",
    " \n",
    "These are described in more detail below.\n",
    "\n",
    "\n",
    "## TestPool\n",
    "\n",
    "The TestPool consists of a small number of cores, available for anyone to use at any time, even without an allocation. These are meant to be used for jobs with quick turnaround time, such as tutorials, development, and debugging. \n",
    "\n",
    "You can see the state of the TestPool by running:\n",
    "\n",
    "`condor_status -const 'TestPool =?= True'`\n",
    "\n",
    "If you do not want your jobs to run on the TestPool, please add `TestPool =!= True` to your job requirements. \n",
    "\n",
    "\n",
    "## Cloud\n",
    "\n",
    "Adding cloud resources, using your own allocation, is done by starting a provided VM image, and injecting a provided token for authentication. The VMs join the pool and start running jobs. When there are no more jobs, the VMs shut themselves down. \n",
    "\n",
    "[More details on how to provide cloud resources](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html#Cloud)\n",
    "\n",
    "\n",
    "## HTCondor Annex\n",
    "\n",
    "ACCESS HPC Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the clusters. The pilots will run under your ACCESS allocation, and have the following properties:\n",
    "\n",
    " 1. **A pilot can run multiple user jobs** - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    " 2. **A pilot is partitionable** - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    " 3. **A pilot will only run jobs for the user who started it.**\n",
    "\n",
    "Annexes can be named, and jobs can be configured to only go to certain named Annexes. By default, the annexes are named with your username.\n",
    "\n",
    "[More details on how to provide annex resources](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html#HTCondor-Annex)\n",
    "\n",
    "## OSPool\n",
    "\n",
    "The OSPool is always connected to ACCESS Pegasus, but requires jobs to have an OSG project name specified. If you have an ACCESS allocation on OSG, you can use the “TG-NNNNNN” allocation id as project name. Or, if you have an OSG assigned project name, you may use that. You can specify the project name in your workflows like:\n",
    "\n",
    "    props.add_site_profile(\"condorpool\", \"condor\", \"+ProjectName\", \"\\\"TG-NNNNNN\\\"\")\n",
    "\n",
    "Also note that the OSPool uses a different approach to containers. Instead of using Pegasus’ built in container execution, create non-container jobs, with a property specify the container to use:\n",
    "\n",
    "    props.add_site_profile(\"condorpool\", \"condor\", \"+SingularityImage\", \"\\\"/cvmfs/singularity.opensciencegrid.org/htc/rocky:8\\\"\")\n",
    "\n",
    "More information about containers on the OSPool can be found in the [OSG documentation](https://portal.osg-htc.org/documentation/htc_workloads/using_software/containers-singularity/)\n",
    "\n",
    "More details on how to the OSPool\n",
    "[More details on how to use the OSPool](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html#OSPool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
