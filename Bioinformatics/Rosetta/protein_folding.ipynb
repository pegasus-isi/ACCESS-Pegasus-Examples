{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768c194e",
   "metadata": {},
   "source": [
    "# Rosetta Protein-folding workflow\n",
    "\n",
    "This is a Pegasus workflow for running Rosetta's De novo structure prediction on the OSG. The workflow predicts the 3-dimensional structure of a protein starting with an amino acid sequence, using the [Abinitio Relax](https://new.rosettacommons.org/docs/latest/application_documentation/structure_prediction/abinitio-relax#algorithm) algorithm. This workflow uses ideas from this [tutorial](https://www.rosettacommons.org/demos/latest/tutorials/denovo_structure_prediction/Denovo_structure_prediction).\n",
    "\n",
    "> Please run the workflow from your [OSG Connect](https://www.osgconnect.net) account. Anyone with a U.S. research affiliation can get access.\n",
    "\n",
    "## Configure Input files\n",
    "You will need to have a license to download Rosetta. See the [Rosetta documentation](https://www.rosettacommons.org/demos/latest/tutorials/install_build/install_build) for details on how to obtain the license. Once you have the license, you can download the Rosetta software suite from https://www.rosettacommons.org/software/license-and-download.\n",
    "\n",
    "Untar the downloaded file by running this command in your terminal:\n",
    "\n",
    "```tar -xvzf rosetta[releasenumber].tar.gz```\n",
    "\n",
    "### Binaries\n",
    "\n",
    "The ab initio executable can be found in ```rosetta*/main/source/bin```. Navigate to this directory and copy the AbinitioRelax file to the ```bin``` directory of the rosetta_workflow. Make sure the file name in the last line of proteinfold.sh matches the one you copied. \n",
    "\n",
    "### Database\n",
    "The Pegasus workflow takes as input the database as a tarball file. Create the tar file of the database folder found in ```rosetta*/main``` and place it in the ```database``` directory of the workflow. \n",
    "\n",
    "```cd [path to rosetta*]/main/ && tar -czf [path to rosetta workflow]/database/database.tar.gz database```\n",
    "\n",
    "### Data inputs\n",
    "A job in the rosetta workflow requires the following input files for an amino acid sequence:\n",
    "\n",
    "* Fasta file - Example: 1elwA.fasta\n",
    "\n",
    "* Fragments files - Example: aa1elwA03_05.200_v1_3 and aa1elwA09_05.200_v1_3\n",
    "\n",
    "* PDB file. Example - 1elw.pdb\n",
    "\n",
    "* Psipred secondary structure prediction psipred_ss2 file - Example: 1elwA.psipred_ss2\n",
    "\n",
    "> **Note**: Rename the input files to have the same base name. \n",
    ">               \n",
    "> Example: data-1.fasta, data-1.pdb, data-1.psipred_ss2, data-1-09_05.200_v1_3, data-1-03_05.200_v1_3 and the folder containing these input files as data-1.\n",
    "\n",
    "Run the command on the folder ```data-<i>``` containing the above input files for a sequence\n",
    "\n",
    "```tar -cf data-<i>.tar.gz data-<i> ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf85552",
   "metadata": {},
   "source": [
    "### 1. Creating the workflow\n",
    "We use Pegasus Workflow API to create the workflow for Rosetta's protein-folding for structure prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd923ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Working Directory Setup --------------------------------------------------\n",
    "# A good working directory for workflow runs and output files\n",
    "WORK_DIR = Path.home() / \"workflows\"\n",
    "WORK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TOP_DIR = Path().resolve()\n",
    "\n",
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.data.configuration\"] = \"condorio\"  \n",
    "\n",
    "# Provide a full kickstart record, including the environment, even for successful jobs\n",
    "props[\"pegasus.gridstart.arguments\"] = \"-f\"\n",
    "\n",
    "#Limit the number of idle jobs for large workflows\n",
    "props[\"dagman.maxidle\"] = \"1600\"\n",
    "\n",
    "# Help Pegasus developers by sharing performance data (optional)\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"\n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "\n",
    "# write properties file to ./pegasus.properties\n",
    "props.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc721c9c",
   "metadata": {},
   "source": [
    "### 2. Site Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sites --------------------------------------------------------------------\n",
    "sc = SiteCatalog()\n",
    "\n",
    "# local site (submit machine)\n",
    "local_site = Site(name=\"local\", arch=Arch.X86_64)\n",
    "\n",
    "local_shared_scratch = Directory(directory_type=Directory.SHARED_SCRATCH, path=WORK_DIR / \"scratch\")\n",
    "local_shared_scratch.add_file_servers(FileServer(url=\"file://\" + str(WORK_DIR / \"scratch\"), operation_type=Operation.ALL))\n",
    "local_site.add_directories(local_shared_scratch)\n",
    "\n",
    "local_storage = Directory(directory_type=Directory.LOCAL_STORAGE, path=TOP_DIR / \"outputs\")\n",
    "local_storage.add_file_servers(FileServer(url=\"file://\" + str(TOP_DIR / \"outputs\"), operation_type=Operation.ALL))\n",
    "local_site.add_directories(local_storage)\n",
    "\n",
    "local_site.add_env(PATH=os.environ[\"PATH\"])\n",
    "sc.add_sites(local_site)\n",
    "\n",
    "# condorpool (execution site)\n",
    "condorpool_site = Site(name=\"condorpool\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "condorpool_site.add_pegasus_profile(style=\"condor\")\n",
    "condorpool_site.add_condor_profile(\n",
    "    universe=\"vanilla\",\n",
    "    request_cpus=3,\n",
    "    request_memory=\"3 GB\",\n",
    "    request_disk=\"10000000\",\n",
    ")\n",
    "\n",
    "sc.add_sites(condorpool_site)\n",
    "\n",
    "# write SiteCatalog to ./sites.yml\n",
    "sc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1247aacf",
   "metadata": {},
   "source": [
    "### 3. Transformation Catalog\n",
    "\n",
    "Note that in the Transformation catalog section of the workflow, the clustering feature is enabled. This tells Pegasus to cluster multiple jobs together.\n",
    "\n",
    "        proteinfold = Transformation(\n",
    "                name=\"proteinfold\",\n",
    "                site=\"local\",\n",
    "                pfn=TOP_DIR / \"bin/proteinfold.sh\",\n",
    "                is_stageable=\"True\",\n",
    "                arch=Arch.X86_64).add_pegasus_profile(clusters_size=10)\n",
    "\n",
    "To disable clustering, set ```clusters_size``` to 1. Experiment with different values for ```clusters_size``` and observe how it affects the time required for the jobs to finish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59641b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformations ----------------------------------------------------------\n",
    "proteinfold = Transformation(\n",
    "    name=\"proteinfold\",\n",
    "    site=\"local\",\n",
    "    pfn=TOP_DIR / \"bin/proteinfold.sh\",\n",
    "    is_stageable=\"True\",\n",
    "    arch=Arch.X86_64).add_pegasus_profile(clusters_size=10)\n",
    "\n",
    "tc = TransformationCatalog()\n",
    "tc.add_transformations(proteinfold)\n",
    "\n",
    "# write TransformationCatalog to ./transformations.yml\n",
    "tc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f24eb3",
   "metadata": {},
   "source": [
    "### 4.Replica Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replicas -----------------------------------------------------------------\n",
    "exec_file = [File(f.name) for f in (TOP_DIR / \"bin\").iterdir() if f.name.startswith(\"AbinitioRelax\")]\n",
    "\n",
    "input_files = [File(f.name) for f in (TOP_DIR / \"inputs\").iterdir()]\n",
    "\n",
    "db_files = [File(f.name) for f in (TOP_DIR / \"database\").iterdir()]\n",
    "\n",
    "rc = ReplicaCatalog()\n",
    "\n",
    "for f in input_files:\n",
    "    rc.add_replica(site=\"local\", lfn=f, pfn=TOP_DIR / \"inputs\" / f.lfn)\n",
    "\n",
    "for f in exec_file:\n",
    "    rc.add_replica(site=\"local\", lfn=f, pfn=TOP_DIR / \"bin\" / f.lfn)\n",
    "\n",
    "for f in db_files:\n",
    "    rc.add_replica(site=\"local\", lfn=f, pfn=TOP_DIR / \"database\" / f.lfn)\n",
    "\n",
    "# write ReplicaCatalog to replicas.yml\n",
    "rc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3f492",
   "metadata": {},
   "source": [
    "### 5. Adding jobs to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf746a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(name=\"protein-folding-workflow\")\n",
    "\n",
    "for f in input_files:\n",
    "    filename = f.lfn.replace(\".tar.gz\",\"\")\n",
    "    out_file = File(filename + \"_silent.out\")\n",
    "\n",
    "    proteinfold_job = Job(proteinfold).add_args(filename, \"-database ./database\",\"-in:file:fasta\",f\"./{filename}.fasta\",\n",
    "            \"-in:file:frag3\",f\"./{filename}-03_05.200_v1_3\",\n",
    "            \"-in:file:frag9\",f\"./{filename}-09_05.200_v1_3\",\"-in:file:native\",f\"./{filename}.pdb\",\n",
    "            \"-abinitio:relax\",\"-nstruct\",\"1\",\n",
    "            \"-out:file:silent\", out_file,\n",
    "            \"-use_filters\",\"true\",\"-psipred_ss2\",f\"./{filename}.psipred_ss2\",\n",
    "            \"-abinitio::increase_cycles\",\"10\",\n",
    "            \"-abinitio::rg_reweight\",\"0.5\",\"-abinitio::rg_reweight\",\"0.5\",\n",
    "            \"-abinitio::rsd_wt_helix\",\"0.5\",\"-abinitio::rsd_wt_loop\",\"0.5\",\"-relax::fast\")\\\n",
    "            .add_inputs(exec_file[0],db_files[0],f).add_outputs(out_file)\n",
    "    wf.add_jobs(proteinfold_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719cda32",
   "metadata": {},
   "source": [
    "### 6. Submit the workflow and launch pilot jons on ACCESS resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a78d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan and run the workflow\n",
    "wf.plan(\n",
    "    dir=WORK_DIR / \"runs\",\n",
    "    sites=[\"condorpool\"],\n",
    "    staging_sites={\"condorpool\":\"local\"},\n",
    "    output_sites=[\"local\"],\n",
    "    cluster=[\"horizontal\"],\n",
    "    submit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465c0e4",
   "metadata": {},
   "source": [
    "Note that we are running jobs on site condorpool i.e the selected ACCESS resource. After the workflow has been successfully planned and submitted, you can use ```wf.status()``` in order to monitor the status of the workflow. It shows in detail the counts of jobs of each status and also the whether the job is idle or running.\n",
    "\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "The process of starting pilots is described in the [ACCESS Pegasus Documentation](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e4410",
   "metadata": {},
   "source": [
    "### 7. Statistics\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use ```wf.analyze()``` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs using ```wf.statistics()```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
