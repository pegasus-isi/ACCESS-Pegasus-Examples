{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Calling Pegasus Workflow\n",
    "\n",
    "In this notebook, we will create Pegasus Workflow corresponding to the  [Automating a Variant Calling Workflow](https://datacarpentry.org/wrangling-genomics/05-automation.html) \n",
    "from Data Carpentry Lesson \n",
    "[Data Wrangling and Processing for Genomics](https://datacarpentry.org/wrangling-genomics/).\n",
    "\n",
    "This workflow downloads and aligns SRA data to the E. coli  REL606 reference genome, and checks what differences exist in our reads versus the genome. The workflow also performs perform  variant calling to see how \n",
    "the population changed over time. \n",
    "\n",
    "One major change from other examples is that this workflow, uses Open Storage Network (OSN) to do the\n",
    "data staging. The previous examples, uses the directory space on **access.pegasus.isi.edu** which is\n",
    "limited.\n",
    "\n",
    "## Container\n",
    "\n",
    "All tools required to execute the jobs in the container are all included in\n",
    "a single Docker container defined in `docker/Dockerfile` and available in the\n",
    "[Docker Hub](https://hub.docker.com/repository/docker/pegasus/variant-calling) under\n",
    "`pegasus/variant-calling`. The workflow is setup up to use that container\n",
    "but execute it via Singularity as that maybe a more common container\n",
    "runtime on HPC machines. The container runtime used can easily be\n",
    "changed in the workflow definition.\n",
    "\n",
    "The container comes with the following tools\n",
    "* Burrows-Wheeler Aligner (BWA) 0.7.17\n",
    "* SamTools 1.15.1\n",
    "* Bcftools 1.15.1\n",
    "* HTSlib   1.15.1\n",
    "* SRA Tools 3.0.0\n",
    "\n",
    "The number of concurrent downloads is limited with a DAGMan\n",
    "category profile.\n",
    "\n",
    "## OSN Access\n",
    "\n",
    "The Open Storage Network (OSN) is a distributed data sharing and transfer service intended to facilitate exchanges of active scientific data sets between research organizations, communities and projects, providing easy access and high bandwidth delivery of large data sets to researchers.\n",
    "\n",
    "The OSN serves two principal purposes: (1) enable the smooth flow of large data sets between resources such as instruments, campus data centers, national supercomputing centers, and cloud providers; and (2) facilitate access to long tail data sets by the scientific community.  \n",
    "\n",
    "If you dont have access to an existing OSN bucket. Please refer to the instructions on the \n",
    "[OSN Page In ACCESS WIKI](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/Open+Storage+Network+OSN.html)\n",
    "\n",
    "## Workflow\n",
    "\n",
    "The Pegasus workflow downloads SRA data from NCBI repository using\n",
    "`fasterq-dump` in the SRA toolkit and aligns it against the reference \n",
    "genome.\n",
    "\n",
    "![Pegasus Variant Calling Workflow for 2 SRA reads](../../images/variant-calling-pegasus-workflow.png)\n",
    "\n",
    "The tools used for various jobs in the worklfow are listed in table below\n",
    "\n",
    "| Job Label                 | Tool Used        |\n",
    "| --------------------------|----------------- |\n",
    "| bwa                       | bwa              |\n",
    "| fasterq_dump              | fasterq_dump     |\n",
    "| align_reads               | bwa              |\n",
    "| sam_2_bam_converter       | samtools         |\n",
    "| calculate_read_coverage   | bcftools         |\n",
    "| detect_snv                | bcftools         |\n",
    "| variant_calling           | vcfutils         |\n",
    "\n",
    "The bwa invocation as part of the align_reads job is configured to use multiple cores. In this notebook, we set the core usage for that job to 6 and enable use of threads( set to 12 - an oversubscription) on the command line to the bwa job. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up Credentials for OSN\n",
    "\n",
    "As a first step, you need to specify your access key and secret key in the Pegasus credentials file. \n",
    "\n",
    "This file resides at ~/.pegasus/credentials.conf\n",
    "\n",
    "Open this file in your favorite editor (vim, emacs, nano) and put in the following\n",
    "\n",
    "```\n",
    "\n",
    "[osn]\n",
    "endpoint = https://sdsc.osn.xsede.org\n",
    "\n",
    "[USER@osn]\n",
    "access_key = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "secret_key = abababababababababababababababab\n",
    "\n",
    "```\n",
    "\n",
    "**Note** Replace USER with your ACCESS username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.pegasus/credentials.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the Variant Calling Workflow\n",
    "\n",
    "By now, you have a good idea about the Pegasus Workflow API.\n",
    "We now create the workflow for the Variant Calling based on the picture above\n",
    "\n",
    "First step is to identify what SRA we want to pull down from the NCBI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sra_list = [\"SRR2584866\",\n",
    "            \"SRR2589044\"]\n",
    "reference_genome=\"./ref_genome/ecoli_rel606.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the variables below to indicate the OSN bucket that you have access to, with access key and the secret key listed in the Pegasus credentials file. \n",
    "\n",
    "The user variable should point to your ACCESS user name and be consistent with the entry in the Pegasus credentials file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update to a OSN bucket you have access to. For example asc190064-bucket01 \n",
    "osn_bucket=\"BUCKET\" \n",
    "# update to your ACCESS username\n",
    "access_user=\"USER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "BASE_DIR = str(Path(\".\").resolve())\n",
    "\n",
    "# need to know where Pegasus is installed for notifications\n",
    "PEGASUS_HOME = shutil.which('pegasus-version')\n",
    "PEGASUS_HOME = os.path.dirname(os.path.dirname(PEGASUS_HOME))\n",
    "\n",
    "wf = Workflow('variant-calling')\n",
    "tc = TransformationCatalog()\n",
    "rc = ReplicaCatalog()\n",
    "sc = SiteCatalog()\n",
    "    \n",
    "# --- Properties ----------------------------------------------------------\n",
    "\n",
    "# set the concurrency limit for the download jobs, and send some extra usage\n",
    "# data to the Pegasus developers\n",
    "props = Properties()\n",
    "props['dagman.fasterq-dump.maxjobs'] = '20'\n",
    "props['pegasus.catalog.workflow.amqp.url'] = 'amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows'\n",
    "props['pegasus.data.configuration'] = 'nonsharedfs'\n",
    "props.write() \n",
    "\n",
    "# --- Event Hooks ---------------------------------------------------------\n",
    "\n",
    "# get emails on all events at the workflow level\n",
    "wf.add_shell_hook(EventType.ALL, '{}/share/pegasus/notification/email'.format(PEGASUS_HOME))\n",
    "\n",
    "# --- Transformations -----------------------------------------------------\n",
    "\n",
    "container = Container(\n",
    "               'variant-calling',\n",
    "               Container.SINGULARITY,\n",
    "               'docker://pegasus/variant-calling:latest'\n",
    "            )\n",
    "tc.add_containers(container)\n",
    "\n",
    "fasterq_dump = Transformation(\n",
    "    'fasterq-dump',\n",
    "    site='local',\n",
    "    container=container,\n",
    "    pfn=BASE_DIR + '/tools/fasterq_dump_wrapper',\n",
    "    is_stageable=True\n",
    ")\n",
    "fasterq_dump.add_profiles(Namespace.CONDOR, key='request_memory', value='1 GB')\n",
    "# this one is used to limit the number of concurrent downloads\n",
    "fasterq_dump.add_profiles(Namespace.DAGMAN, key='category', value='fasterq-dump')\n",
    "tc.add_transformations(fasterq_dump)\n",
    "\n",
    "bwa = Transformation(\n",
    "                   'bwa',\n",
    "                   site='incontainer',\n",
    "                   container=container,\n",
    "                   pfn='/opt/software/install/bwa/default/bwa',\n",
    "                   is_stageable=False\n",
    "                )\n",
    "bwa.add_profiles(Namespace.CONDOR, key='request_memory', value='1 GB')\n",
    "tc.add_transformations(bwa)\n",
    "\n",
    "# we use the simple bash wrapper to convert to bam,\n",
    "# sort and index the generated bam file\n",
    "samtools = Transformation(\n",
    "    'samtools',\n",
    "    site='local',\n",
    "    container=container,\n",
    "    pfn=BASE_DIR + '/tools/samtools_wrapper',\n",
    "    is_stageable=True\n",
    ")\n",
    "samtools.add_profiles(Namespace.CONDOR, key='request_memory', value='2 GB')\n",
    "tc.add_transformations(samtools)\n",
    "\n",
    "bcftools = Transformation(\n",
    "    'bcftools',\n",
    "    site='incontainer',\n",
    "    container=container,\n",
    "    pfn='/opt/software/install/bcftools/default/bin/bcftools',\n",
    "    is_stageable=False\n",
    ")\n",
    "bcftools.add_profiles(Namespace.CONDOR, key='request_memory', value='1 GB')\n",
    "tc.add_transformations(bcftools)\n",
    "\n",
    "vcfutils = Transformation(\n",
    "    'vcfutils',\n",
    "    site='incontainer',\n",
    "    container=container,\n",
    "    pfn='/opt/software/install/bcftools/default/bin/vcfutils.pl',\n",
    "    is_stageable=False\n",
    ")\n",
    "vcfutils.add_profiles(Namespace.CONDOR, key='request_memory', value='1 GB')\n",
    "tc.add_transformations(vcfutils)\n",
    "\n",
    "# --- Site Catalog ------------------------------------------------- \n",
    "osn = Site(\"osn\", arch=Arch.X86_64, os_type=OS.LINUX)\n",
    "\n",
    "# create and add a bucket in OSN to use for your workflows\n",
    "osn_shared_scratch_dir = Directory(Directory.SHARED_SCRATCH, path=\"/\" + osn_bucket + \"/pegasus-workflows/variant\") \\\n",
    "    .add_file_servers(\n",
    "    FileServer(\"s3://\" + access_user +\"@osn/\" + osn_bucket + \"/pegasus-workflows/variant\", Operation.ALL),\n",
    ")\n",
    "osn.add_directories(osn_shared_scratch_dir)\n",
    "sc.add_sites(osn)\n",
    "\n",
    "# add a local site with an optional job env file to use for compute jobs\n",
    "shared_scratch_dir = \"{}/work\".format(BASE_DIR)\n",
    "local_storage_dir = \"{}/storage\".format(BASE_DIR)\n",
    "local = Site(\"local\") \\\n",
    "    .add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "        .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL)))\n",
    "\n",
    "job_env_file = Path(str(BASE_DIR) + \"/../tools/job-env-setup.sh\").resolve()\n",
    "local.add_pegasus_profile(pegasus_lite_env_source=job_env_file)\n",
    "\n",
    "sc.add_sites(local)\n",
    "\n",
    "# --- Workflow -----------------------------------------------------\n",
    "# set up the reference genome and what files need to be generated by the index job\n",
    "ref_genome_lfn=os.path.basename(reference_genome)\n",
    "ref_genome = File(ref_genome_lfn)\n",
    "rc.add_replica('local', ref_genome_lfn, os.path.abspath(reference_genome))\n",
    "index_files = []\n",
    "for suffix in ['amb', 'ann', 'bwt', 'pac', 'sa']:\n",
    "    index_files.append(File(ref_genome.lfn + \".\" + suffix))\n",
    "\n",
    "# index the reference file\n",
    "index_job = Job('bwa', node_label=\"ref_genome_index\")\n",
    "index_job.add_args('index', ref_genome.lfn)\n",
    "index_job.add_inputs(ref_genome)\n",
    "index_job.add_outputs(*index_files, stage_out=False)\n",
    "wf.add_jobs(index_job)\n",
    "\n",
    "# create jobs for each trimmed fastq trim.sub.fastq\n",
    "for sra in sra_list:\n",
    "    sra_id = sra.strip()\n",
    "    if len(sra_id) < 5:\n",
    "        continue\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # files for this id\n",
    "    # commented out as we download files from NCBI as part of fasterq-dump job\n",
    "    fastq_1 = File('{}_1.trim.sub.fastq'.format(sra_id))\n",
    "    fastq_2 = File('{}_2.trim.sub.fastq'.format(sra_id))\n",
    "    rc.add_replica('local', fastq_1, os.path.join(os.path.abspath(args.fastq_dir), fastq_1.lfn))\n",
    "    rc.add_replica('local', fastq_2, os.path.join(os.path.abspath(args.fastq_dir), fastq_2.lfn))\n",
    "    \"\"\"\n",
    "\n",
    "    sam=File('{}.aligned.sam'.format(sra_id))\n",
    "    bam=File('{}.aligned.bam'.format(sra_id))\n",
    "    sorted_bam=File('{}.aligned.sorted.bam'.format(sra_id))\n",
    "\n",
    "    raw_bcf=File('{}_raw.bcf'.format(sra_id))\n",
    "    variants=File('{}_variants.bcf'.format(sra_id))\n",
    "    final_variants=File('{}_final_variants.bcf'.format(sra_id))\n",
    "\n",
    "    \"\"\"\n",
    "    bwa mem $genome $fq1 $fq2 > $sam\n",
    "    samtools view -S -b $sam > $bam\n",
    "    samtools sort -o $sorted_bam $bam \n",
    "    samtools index $sorted_bam\n",
    "    bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\n",
    "    bcftools call --ploidy 1 -m -v -o $variants $raw_bcf \n",
    "    vcfutils.pl varFilter $variants > $final_variants\n",
    "    \"\"\"\n",
    "\n",
    "    # files for this id\n",
    "    fastq_1 = File('{}_1.fastq'.format(sra_id))\n",
    "    fastq_2 = File('{}_2.fastq'.format(sra_id))\n",
    "\n",
    "    # download job\n",
    "    j = Job('fasterq-dump', node_label=\"fasterq_dump\")\n",
    "    j.add_args('--split-files', sra_id)\n",
    "    j.add_outputs(fastq_1, fastq_2, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # align reads tp reference genome job\n",
    "    j = Job('bwa', node_label=\"align_reads\")\n",
    "    # Note that the cores we give Pegasus and the -t does not match.\n",
    "    # Oversubscriptions is ok, as bwa can not keep all the cores busy 100%\n",
    "    # of the time.\n",
    "    j.add_pegasus_profile(cores=6)\n",
    "    j.add_args('mem', \"-t 12\", ref_genome, fastq_1, fastq_2)\n",
    "    j.add_inputs(*index_files, ref_genome, fastq_1, fastq_2)\n",
    "    j.set_stdout(sam, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # samtools_wrapper for doing alignment to genome\n",
    "    j = Job('samtools', node_label=\"sam_2_bam_converter\")\n",
    "    j.add_args(sra_id)\n",
    "    j.add_inputs(sam)\n",
    "    j.add_outputs(bam, sorted_bam, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # Variant calling\n",
    "    # bcftools for calculating the read coverage of positions in the genome\n",
    "    j = Job('bcftools', node_label=\"calculate_read_coverage\")\n",
    "    j.add_args('mpileup -O b -o', raw_bcf, '-f', ref_genome, sorted_bam)\n",
    "    j.add_inputs(ref_genome, sorted_bam)\n",
    "    j.add_outputs(raw_bcf, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # bcftools for Detect the single nucleotide variants (SNVs)\n",
    "    j = Job('bcftools', node_label=\"detect_snv\")\n",
    "    j.add_args('call --ploidy 1 -m -v -o', variants, raw_bcf)\n",
    "    j.add_inputs(raw_bcf)\n",
    "    j.add_outputs(variants, stage_out=False)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "    # vcfutils Filter and report the SNV variants in variant calling format (VCF)\n",
    "    j = Job('vcfutils', node_label=\"variant_calling\")\n",
    "    j.add_args('varFilter', variants)\n",
    "    j.add_inputs(variants)\n",
    "    j.set_stdout(final_variants, stage_out=True)\n",
    "    wf.add_jobs(j)\n",
    "\n",
    "wf.add_transformation_catalog(tc)\n",
    "wf.add_site_catalog(sc)\n",
    "wf.add_replica_catalog(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Workflow\n",
    "\n",
    "Once you have defined your abstract workflow, you can use `pegasus-graphviz` to visualize it. `Workflow.graph()` will invoke `pegasus-graphviz` internally and render your workflow using one of the available formats such as `png`. **Note that Workflow.write() must be invoked before calling Workflow.graph().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. You will notice below that we have added a new option staging_sites to plan the workflow. This option tells Pegasus to use **osn** site for data staging when running jobs on site **condorpool**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(staging_sites={\"condorpool\": \"osn\"}, sites=[\"condorpool\"], submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line in the output that starts with pegasus-status, contains the command you can use to monitor the status of the workflow. We will cover this command line tool in the next couple of notbooks. The path it contains is the path to the submit directory where all of the files required to submit and monitor the workflow are stored. For now we will just continue to use the Python `Workflow` object.\n",
    "\n",
    "## 5.  Launch Pilots Jobs on ACCESS resources\n",
    "\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "A pilot can run multiple user jobs - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    "\n",
    "A pilot is partitionable - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    "\n",
    "A pilot will only run jobs for the user who started it.\n",
    "\n",
    "The process of starting pilots is described in the [ACCESS Pegasus Documentation](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html)\n",
    "\n",
    "## 6. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  What's next\n",
    "\n",
    "Next Notebook is `05-Summary`, that summarizes what we have learnt so far and how to request further support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
