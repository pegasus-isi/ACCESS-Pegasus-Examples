{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1598debe",
   "metadata": {},
   "source": [
    "# Lung Segmentation Pegasus Workflow\n",
    "\n",
    "Precise detection of the borders of organs and lesions in medical images such as X-rays, CT, or MRI scans is an essential step towards correct diagnosis and treatment planning. We implement a workflow that employs supervised learning techniques to locate lungs on X-ray images. Lung instance segmentation workflow uses [Chest X-ray](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/) for predicting lung masks from the images using [U-Net](https://arxiv.org/abs/1505.04597) model.\n",
    "\n",
    "The workflow uses a **Chest X-ray Masks** and Labels dataset (high-resolution X-ray images and masks) availabe publicly. The dataset is split into training, validation, and test sets before the workflow starts. Each set consists of original lung images and their associated masks. The **Pre-processing** step and Data Augmentation of Images is done to resize images (lungs and masks) and normalize lung X-rays. Additionally, for each pair of lung image and mask in the train dataset, two new pairs are generated through **image augmentation** (e.g., rotations, flips). Next, the train and validation data are passed to the UNet **hyperparameter optimization** step, where different learning rates are explored. The **training** of UNet fine-tunes the UNet model with the recommended learning rate on the concatenated train and validation set, and obtains the weights. Then **inference** on Unet is done using the trained model to generate masks for the test X-ray images. Finally, the **evaluation** is performed in order to generate a PDF file with the scores for relevant performance metrics and prints examples of lung segmentation images produced by the model.\n",
    "\n",
    "![Lung Segmentation](img/segmentation.png)\n",
    "\n",
    "**Machine Learning steps in the workflow :**\n",
    "<br>\n",
    "<img src=\"img/ml_steps.png\" style=\"width: 850px;\"/>\n",
    "<br>\n",
    "\n",
    "## Container\n",
    "All tools required to execute the jobs are all included in the container available on Dockerhub :\n",
    "<br>[Lung Segmentation Container](https://hub.docker.com/r/papajim/lung-segmentation) which runs on python and uses  machine learning libraries defined in `Docker/Dockerfile` as -\n",
    "* scikit-learn \n",
    "* tensorflow==2.1.0\n",
    "* h5py \n",
    "* numpy==1.18.4 \n",
    "* pandas \n",
    "* opencv-python \n",
    "* keras==2.3.1 \n",
    "* optuna \n",
    "* segmentation_models\n",
    "* matplotlib\n",
    "\n",
    "## Input Data\n",
    "Sample input data has been provided in `inputs` containing images and masks for training and testing.\n",
    "<br>`inputs/train_images` **:** consists of 512x512 chest x-ray images for training\n",
    "<br>`inputs/train_masks` **:** consists of 512x512 lung masks for training\n",
    "<br>`inputs/test_images` **:** consists of 256x256 chest x-ray images for testing\n",
    "\n",
    "\n",
    "## Workflow\n",
    "The workflow pre-processes the input data and then trains machine learning model to automatically predict lung masks.\n",
    "\n",
    "<img src=\"img/workflow.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "<br>The descriptions for various jobs in the worklfow are listed in a table below\n",
    "\n",
    "| Job Label         | Description                                              |\n",
    "| ------------------|----------------------------------------------------------|\n",
    "| preprocess_test   | data preprocessing for the testing set of x-ray images   |\n",
    "| preprocess_val    | data preprocessing for the validation set of x-ray images|\n",
    "| hpo               | hyperparameter optimization step for UNet model          |\n",
    "| train_model       | training the UNet model and fine-tuning it               |\n",
    "| predict_masks     | predicting the lung masks                                |\n",
    "| evaluate          | generates scores for relevant performance metrics        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538152e",
   "metadata": {},
   "source": [
    "## 1. Create the Lung Segmentation Workflow\n",
    "\n",
    "By now, you have a good idea about the Pegasus Workflow API.\n",
    "We now create the workflow for the Lung segmentation based on the picture above.\n",
    "\n",
    "All workflow parameters are have been set along with input dataset values. This workflow is running on the sample dataset, which is included in the repository under `inputs` directory. The workflow parameters and input files location are set in the beginning of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc60ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import math\n",
    "import sys, os\n",
    "from argparse import ArgumentParser\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "log.basicConfig(level=log.INFO)\n",
    "\n",
    "# --- Import Pegasus API -----------------------------------------------------------\n",
    "from Pegasus.api import *\n",
    "\n",
    "# --- Top Directory Setup ----------------------------------------------------------\n",
    "top_dir = Path(__file__).parent.resolve()\n",
    "\n",
    "\n",
    "######################## WORKFLOW PARAMETERS ########################\n",
    "IGNORE_IMAGES = {'CHNCXR_0025_0.png', 'CHNCXR_0036_0.png', 'CHNCXR_0037_0.png', 'CHNCXR_0038_0.png', 'CHNCXR_0039_0.png', 'CHNCXR_0040_0.png', 'CHNCXR_0065_0.png', 'CHNCXR_0181_0.png', 'CHNCXR_0182_0.png', 'CHNCXR_0183_0.png', 'CHNCXR_0184_0.png', 'CHNCXR_0185_0.png', 'CHNCXR_0186_0.png', 'CHNCXR_0187_0.png', 'CHNCXR_0188_0.png', 'CHNCXR_0189_0.png', 'CHNCXR_0190_0.png', 'CHNCXR_0191_0.png', 'CHNCXR_0192_0.png', 'CHNCXR_0193_0.png', 'CHNCXR_0194_0.png', 'CHNCXR_0195_0.png', 'CHNCXR_0196_0.png', 'CHNCXR_0197_0.png', 'CHNCXR_0198_0.png', 'CHNCXR_0199_0.png', 'CHNCXR_0200_0.png', 'CHNCXR_0201_0.png', 'CHNCXR_0202_0.png', 'CHNCXR_0203_0.png', 'CHNCXR_0204_0.png', 'CHNCXR_0205_0.png', 'CHNCXR_0206_0.png', 'CHNCXR_0207_0.png', 'CHNCXR_0208_0.png', 'CHNCXR_0209_0.png', 'CHNCXR_0210_0.png', 'CHNCXR_0211_0.png', 'CHNCXR_0212_0.png', 'CHNCXR_0213_0.png', 'CHNCXR_0214_0.png', 'CHNCXR_0215_0.png', 'CHNCXR_0216_0.png', 'CHNCXR_0217_0.png', 'CHNCXR_0218_0.png', 'CHNCXR_0219_0.png', 'CHNCXR_0220_0.png', 'CHNCXR_0336_1.png', 'CHNCXR_0341_1.png', 'CHNCXR_0342_1.png', 'CHNCXR_0343_1.png', 'CHNCXR_0344_1.png', 'CHNCXR_0345_1.png', 'CHNCXR_0346_1.png', 'CHNCXR_0347_1.png', 'CHNCXR_0348_1.png', 'CHNCXR_0349_1.png', 'CHNCXR_0350_1.png', 'CHNCXR_0351_1.png', 'CHNCXR_0352_1.png', 'CHNCXR_0353_1.png', 'CHNCXR_0354_1.png', 'CHNCXR_0355_1.png', 'CHNCXR_0356_1.png', 'CHNCXR_0357_1.png', 'CHNCXR_0358_1.png', 'CHNCXR_0359_1.png', 'CHNCXR_0360_1.png', 'CHNCXR_0481_1.png', 'CHNCXR_0482_1.png', 'CHNCXR_0483_1.png', 'CHNCXR_0484_1.png', 'CHNCXR_0485_1.png', 'CHNCXR_0486_1.png', 'CHNCXR_0487_1.png', 'CHNCXR_0488_1.png', 'CHNCXR_0489_1.png', 'CHNCXR_0490_1.png', 'CHNCXR_0491_1.png', 'CHNCXR_0492_1.png', 'CHNCXR_0493_1.png', 'CHNCXR_0494_1.png', 'CHNCXR_0495_1.png', 'CHNCXR_0496_1.png', 'CHNCXR_0497_1.png', 'CHNCXR_0498_1.png', 'CHNCXR_0499_1.png', 'CHNCXR_0500_1.png', 'CHNCXR_0502_1.png', 'CHNCXR_0505_1.png', 'CHNCXR_0560_1.png', 'CHNCXR_0561_1.png', 'CHNCXR_0562_1.png', 'CHNCXR_0563_1.png', 'CHNCXR_0564_1.png', 'CHNCXR_0565_1.png'}\n",
    "NUM_OF_HPO_JOBS = 1\n",
    "num_inputs = 1\n",
    "gpus = False\n",
    "\n",
    "# --- Get input files --------------------------------------------------------------\n",
    "lung_img_dir = Path(\"inputs/train_images\")\n",
    "lung_mask_img_dir = Path(\"inputs/train_masks\")\n",
    "\n",
    "\n",
    "# --- Data Preprocessing function --------------------------------------------------\n",
    "def train_test_val_split(preprocess, training_input_files, mask_files, processed_training_files, processed_val_files, processed_test_files, training_masks, val_masks, test_masks, num_inputs):\n",
    "    np.random.seed(4)\n",
    "    process_jobs = [Job(preprocess).add_args(\"--type\", group) for group in [\"train\", \"val\", \"test\"]]\n",
    "    augmented_masks = []\n",
    "\n",
    "\n",
    "    # --- Write ReplicaCatalog -----------------------------------------------------\n",
    "    rc = ReplicaCatalog()\n",
    "\n",
    "    # add mask images to rc\n",
    "    for f in LUNG_MASK_IMG_DIR.iterdir():\n",
    "        if f.name.endswith(\".png\"):\n",
    "            if f.name in IGNORE_IMAGES:\n",
    "                continue\n",
    "            \n",
    "            mask_files.append(File(f.name))\n",
    "            rc.add_replica(site=\"local\", lfn=f.name, pfn=f.resolve())\n",
    "    \n",
    "    #add an empty(probably checkpoint file\n",
    "    #checkpoint files  and results (empty one should be given if none exists)\n",
    "    for fname in [\"inputs/checkpoints/study_checkpoint.pkl\", \"bin/model/unet.py\", \"bin/model/utils.py\"]:\n",
    "        p = Path(__file__).parent.resolve() / fname\n",
    "        if not p.exists():\n",
    "            with open(p, \"w\") as dummyFile:\n",
    "                dummyFile.write(\"\")\n",
    "        replicaFile = File(p.name)\n",
    "        rc.add_replica(site=\"local\", lfn=replicaFile, pfn=p)\n",
    "\n",
    "    for f in LUNG_IMG_DIR.iterdir():\n",
    "        if f.name.endswith(\".png\") and (\"mask\" not in f.name.lower()) and (f.name not in IGNORE_IMAGES):\n",
    "            training_input_files.append(f)\n",
    "\n",
    "    random.shuffle(training_input_files)\n",
    "    l = len(training_input_files) if num_inputs == -1 else num_inputs\n",
    "    print('Length ', l)\n",
    "\n",
    "    i = 0\n",
    "    for file in training_input_files:\n",
    "        if i+1 <= 0.7*l:\n",
    "            f = File(\"train_{}\".format(file.name))\n",
    "            rc.add_replica(site=\"local\", lfn=f, pfn=file.resolve()) \n",
    "\n",
    "            process_jobs[0].add_inputs(f)\n",
    "            log.info(\"preprocess_train adding input {}\".format(f))\n",
    "            op_file1 = File(f.lfn.replace(\".png\", \"_norm.png\"))\n",
    "            op_file2 = File(f.lfn.replace(\".png\", \"_0_norm.png\"))\n",
    "            op_file3 = File(f.lfn.replace(\".png\", \"_1_norm.png\"))\n",
    "            op_mask2 = File(file.name.replace(\".png\", \"_0_mask.png\"))\n",
    "            op_mask3 = File(file.name.replace(\".png\", \"_1_mask.png\"))\n",
    "\n",
    "            for m in mask_files:\n",
    "                mname = m.lfn[0:-9]\n",
    "                if file.name[0:-4] == mname:\n",
    "                    training_masks.append(m)\n",
    "                    break\n",
    "\n",
    "            process_jobs[0].add_outputs(op_file1, op_file2, op_file3, op_mask2, op_mask3)\n",
    "            augmented_masks.extend([op_mask2, op_mask3])\n",
    "            processed_training_files.extend([op_file1, op_file2, op_file3])\n",
    "\n",
    "        elif i+1 <= 0.9*l:\n",
    "            f = File(\"val_{}\".format(file.name))\n",
    "            rc.add_replica(site=\"local\", lfn=f, pfn=file.resolve())\n",
    "\n",
    "            process_jobs[1].add_inputs(f)\n",
    "            log.info(\"preprocess_val adding input {}\".format(f))\n",
    "            op_file = File(f.lfn.replace(\".png\", \"_norm.png\"))\n",
    "            for m in mask_files:\n",
    "                mname = m.lfn[0:-9]\n",
    "                if file.name[0:-4] == mname:\n",
    "                    val_masks.append(m)\n",
    "                    break\n",
    "                    \n",
    "            process_jobs[1].add_outputs(op_file)\n",
    "            processed_val_files.append(op_file)\n",
    "\n",
    "        else:\n",
    "            f = File(\"test_{}\".format(file.name))\n",
    "            rc.add_replica(site=\"local\", lfn=f, pfn=file.resolve())\n",
    "\n",
    "            process_jobs[2].add_inputs(f)\n",
    "            op_file = File(f.lfn.replace(\".png\", \"_norm.png\"))\n",
    "            for m in mask_files:\n",
    "                mname = m.lfn[0:-9]\n",
    "                if file.name[0:-4] == mname:\n",
    "                    test_masks.append(m)\n",
    "\n",
    "            process_jobs[2].add_outputs(op_file)\n",
    "            log.info(\"preprocess_test adding input {}\".format(f))\n",
    "            processed_test_files.append(op_file)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    log.info(\"writing rc with {} files collected from: {}\".format(len(training_input_files)+len(mask_files), [LUNG_IMG_DIR, LUNG_MASK_IMG_DIR]))\n",
    "    rc.write()\n",
    "    process_jobs[0].add_inputs(*training_masks)\n",
    "    training_masks.extend(augmented_masks)\n",
    "    return process_jobs\n",
    "\n",
    "\n",
    "\n",
    "# --- Write SiteCatalog --------------------------------------------------------\n",
    "sc = SiteCatalog()\n",
    "shared_scratch_dir = os.path.join(top_dir, \"scratch\")\n",
    "local_storage_dir = os.path.join(top_dir, \"output\")\n",
    "\n",
    "local = Site(\"local\")\\\n",
    "            .add_directories(\n",
    "                Directory(Directory.SHARED_SCRATCH, shared_scratch_dir)\n",
    "                    .add_file_servers(FileServer(\"file://\" + shared_scratch_dir, Operation.ALL)),\n",
    "                Directory(Directory.LOCAL_STORAGE, local_storage_dir)\n",
    "                    .add_file_servers(FileServer(\"file://\" + local_storage_dir, Operation.ALL))\n",
    "            )\n",
    "\n",
    "condorpool = Site(\"condorpool\")\\\n",
    "                .add_pegasus_profile(\n",
    "                    style=\"condor\",\n",
    "                    data_configuration=\"condorio\"\n",
    "                )\\\n",
    "                .add_condor_profile(universe=\"vanilla\")\\\n",
    "                .add_profiles(Namespace.PEGASUS, key=\"data.configuration\", value=\"condorio\")\n",
    "\n",
    "sc.add_sites(local, condorpool)\n",
    "sc.write()\n",
    "\n",
    "\n",
    "# --- Write Properties ---------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.mode\"] = \"development\"\n",
    "props.write()\n",
    "\n",
    "\n",
    "\n",
    "# --- Write TransformationCatalog ----------------------------------------------\n",
    "tc = TransformationCatalog()\n",
    "\n",
    "# all jobs to be run in the following container\n",
    "unet_wf_cont = Container(\t\n",
    "                \"unet_wf_model\",\t\n",
    "                Container.SINGULARITY,\t\n",
    "                    image=\"docker:///papajim/lung-segmentation:latest\",\n",
    "                    image_site=\"docker_hub\"\n",
    "            )\n",
    "\n",
    "tc.add_containers(unet_wf_cont)\n",
    "\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/preprocess/preprocess.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            )\n",
    "\n",
    "unet = Transformation(\n",
    "                \"unet\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/model/unet.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            )\n",
    "\n",
    "utils = Transformation(\n",
    "                \"utils\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/model/utils.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            )\n",
    "\n",
    "hpo_task = Transformation( \n",
    "                \"hpo\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/model/hpo.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            ).add_pegasus_profile(cores=8, runtime=14400)\n",
    "hpo_task.add_profiles(Namespace.CONDOR, key='request_memory', value='8 GB')\n",
    "\n",
    "\n",
    "train_model = Transformation( \n",
    "                \"train_model\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/model/train_model.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            ).add_pegasus_profile(cores=8, runtime=7200)\n",
    "train_model.add_profiles(Namespace.CONDOR, key='request_memory', value='8 GB')\n",
    "\n",
    "predict_masks = Transformation( \n",
    "                \"predict_masks\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/model/prediction.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            ).add_pegasus_profile(cores=8, runtime=3600)\n",
    "\n",
    "\n",
    "evaluate_model = Transformation( \n",
    "                \"evaluate\",\n",
    "                site=\"local\",\n",
    "                pfn=top_dir / \"bin/model/evaluate.py\",\n",
    "                is_stageable=True,\n",
    "                container=unet_wf_cont\n",
    "            )\n",
    "\n",
    "if gpus:\n",
    "    hpo_task.add_pegasus_profile(gpus=1)\n",
    "    train_model.add_pegasus_profile(gpus=1)\n",
    "    predict_masks.add_pegasus_profile(gpus=1)\n",
    "\n",
    "tc.add_transformations(preprocess, hpo_task, train_model, predict_masks, evaluate_model, unet, utils)\n",
    "\n",
    "log.info(\"writing tc with transformations: {}, containers: {}\".format([k for k in tc.transformations], [k for k in tc.containers]))\n",
    "tc.write()\n",
    "\n",
    "\n",
    "# --- Generate and run Workflow ------------------------------------------------\n",
    "wf = Workflow(\"lung-instance-segmentation-wf\")\n",
    "\n",
    "#create preprocess job\n",
    "training_input_files = []\n",
    "mask_files = []\n",
    "processed_training_files = []\n",
    "processed_val_files = []\n",
    "processed_test_files = []\n",
    "training_masks = []\n",
    "val_masks = []\n",
    "test_masks = []\n",
    "process_jobs = train_test_val_split(preprocess, training_input_files, mask_files, processed_training_files, processed_val_files, processed_test_files, training_masks, val_masks, test_masks, num_inputs)\n",
    "wf.add_jobs(*process_jobs)\n",
    "log.info(\"generated 3 preprocess jobs\")\n",
    "\n",
    "# create hpo job\n",
    "log.info(\"generating hpo job\")\n",
    "hpo_checkpoint_result = File(f\"study_checkpoint.pkl\")\n",
    "study_result_list = []\n",
    "unet_file = File(\"unet.py\")\n",
    "study_result = File(\"study_results.txt\")\n",
    "study_result_list.append(study_result)\n",
    "hpo_job = Job(hpo_task)\\\n",
    "            .add_args(\"--results_file\", study_result)\\\n",
    "            .add_inputs(*processed_training_files, *processed_val_files, *training_masks, *val_masks, unet_file)\\\n",
    "            .add_outputs(study_result)\\\n",
    "            .add_checkpoint(hpo_checkpoint_result)\n",
    "wf.add_jobs(hpo_job)\n",
    "\n",
    "# create training job\n",
    "log.info(\"generating train_model job\")\n",
    "model = File(\"model.h5\")\n",
    "utils_file = File(\"utils.py\")\n",
    "train_job = Job(train_model)\\\n",
    "                .add_args(\"--params_file\", study_result_list[0])\\\n",
    "                .add_inputs(study_result_list[0], *processed_training_files, *processed_val_files, *training_masks, *val_masks, unet_file, utils_file)\\\n",
    "                .add_outputs(model)\n",
    "wf.add_jobs(train_job)\n",
    "\n",
    "# create mask prediction job\n",
    "log.info(\"generating prediction job; using {} test lung images\".format(len(processed_test_files)))\n",
    "predicted_masks = [File(\"pred_\"+f.lfn.replace(\".png\", \"_mask.png\")[5:]) for f in processed_test_files]\n",
    "predict_job = Job(predict_masks)\\\n",
    "                .add_inputs(model, *processed_test_files, unet_file)\\\n",
    "                .add_outputs(*predicted_masks)\n",
    "wf.add_jobs(predict_job)\n",
    "\n",
    "# create evalute job\n",
    "pdf_analysis = File(\"EvaluationAnalysis.pdf\")\n",
    "evaluate_job = Job(evaluate_model)\\\n",
    "                .add_inputs(*processed_training_files, *processed_test_files, *predicted_masks, *test_masks, unet_file)\\\n",
    "                .add_outputs(pdf_analysis)\n",
    "wf.add_jobs(evaluate_job)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a3cff",
   "metadata": {},
   "source": [
    "## 2. Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site **condorpool** i.e the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aefba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.plan(submit=True, dir=\"runs\", sites=[\"condorpool\"], output_sites=[\"local\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf10766",
   "metadata": {},
   "source": [
    "After the workflow has been successfully planned and submitted, you can use the Python `Workflow` object in order to monitor the status of the workflow. It shows in detail the counts of jobs of each status and also the whether the job is idle or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b523d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92e6b8",
   "metadata": {},
   "source": [
    "## 3.  Launch Pilots Jobs on ACCESS resources\n",
    "\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "A pilot can run multiple user jobs - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    "\n",
    "A pilot is partitionable - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    "\n",
    "A pilot will only run jobs for the user who started it.\n",
    "\n",
    "The process of starting pilots is described in the [ACCESS Pegasus Documentation](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html)\n",
    "\n",
    "## 4. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc02531",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
