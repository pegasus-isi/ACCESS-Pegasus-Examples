{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92ca2cb",
   "metadata": {},
   "source": [
    "# Mask Detection Pegasus Workflow\n",
    "\n",
    "The following project addresses the problem of determining what percentage of the population is properly wearing masks to better track our collective efforts in preventing the spread of COVID-19 in public spaces. To help solve this problem, we leverage modern deep learning tools such as the Optuna hyper parameter optimization framework and the [FastRCNNPredictor](https://arxiv.org/abs/1506.01497) model. The experiment is organized as a scientific workflow and utilizes the Pegasus Workflow Management System to handle its execution on distributed resources. \n",
    "\n",
    "\n",
    "The workflow uses **images of masks on faces** and **annotations** related to each image as classified as one of the following **three categories** as the main input dataset:\n",
    "* wearing a mask \n",
    "* not wearing a mask\n",
    "* wearing a mask incorrectly\n",
    "\n",
    "<img src=\"imgs/classes.png\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "<img src=\"imgs/sample_output.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "The dataset is split into training, validation, and test sets before the workflow starts.  The **Pre-processing** step and Data Augmentation of Images is done to resize images and normalize them to make sure the data is consistent among all classes and also to avoid class imbalance. Additionally, **image augmentation** is done by injecting Gaussian noise. Next, the train and validation data are passed to the **hyperparameter optimization** step, where different learning rates are explored. The **training** of **FastRCNN** model is done with the recommended learning rate on the concatenated train and validation set, and obtains the weights. Then the **evaluation** is performed test set in order to generate a txt file with the scores for relevant performance metrics like average running loss. Finally, **predictions** can be made with any user input images using the trained model and show mask detection results.\n",
    "\n",
    "**Machine Learning steps in the workflow :**\n",
    "<br>\n",
    "<img src=\"imgs/ml_steps3.png\" style=\"width: 900px;\"/>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"imgs/mask_dectection_wf2.png\" style=\"width: 1000px;\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "## Container\n",
    "All tools required to execute the jobs are all included in the container available on Dockerhub :\n",
    "<br>[Mask Detection Container](https://hub.docker.com/r/zaiyancse/mask-detection) which runs on python and uses  machine learning libraries defined in `bin/Dockerfile` as -\n",
    "* tensorflow==2.1.0\n",
    "* optuna==2.0.0\n",
    "* numpy==1.18.4\n",
    "* torch\n",
    "* pandas \n",
    "* opencv-python\n",
    "* scikit-learn \n",
    "* pytorchtools\n",
    "* matplotlib\n",
    "* Pillow\n",
    "* torchvision\n",
    "* bs4\n",
    "\n",
    "## Input Data\n",
    "Sample input data has been provided in `data` containing images and annotation for training and testing.\n",
    "<br>`inputs/images` **:** consists of images for training related to aforementioned three categories and also predictions on unseen images\n",
    "<br>`inputs/annotations` **:** consists of annotations in xml file per corresponding image the category it belongs to\n",
    "\n",
    "\n",
    "## Workflow\n",
    "The workflow pre-processes the input data and then trains deep learning model to detect masks on faces and then classify them into one of three categories. The following figure shows an overview of the design of the pegasus workflow for mask detection and classification :\n",
    "\n",
    "<img src=\"imgs/wf_graph3.png\" style=\"width: 650px;\"/>\n",
    "\n",
    "<br>The descriptions for various jobs in the worklfow are listed in a table below\n",
    "\n",
    "| Job Label              | Description                                                    |\n",
    "| -----------------------|----------------------------------------------------------------|\n",
    "| preprocess_val         | data preprocessing for the validation set of images            |\n",
    "| preprocess_test        | data preprocessing for the testing set of images               |\n",
    "| preprocess_aug_train   | data augmentation of the training set images                   |\n",
    "| plot_class_distribution| data exploration step to visualize class distribution          |\n",
    "| hpo                    | hyperparameter optimization step for FastRCNN model            |\n",
    "| train_model            | training the FastRCNN model and fine-tuning it                 |\n",
    "| evaluate               | generates relevant performance metrics like running loss       |\n",
    "| predict                | make final prediction for mask detection on a given input image|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d5d01",
   "metadata": {},
   "source": [
    "## 1. Create the Mask Detection Workflow\n",
    "\n",
    "By now, you have a good idea about the Pegasus Workflow API.\n",
    "We now create the workflow for the Mask Detection based on the picture above.\n",
    "\n",
    "All workflow parameters are have been set along with input dataset values. This workflow is running on the sample dataset, which is included in the repository under `data` directory. The workflow parameters and input files location are set in the beginning of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob,os\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from utils.wf import split_data_filenames, create_ann_list,create_augmented_filelist\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# --- Import Pegasus API -----------------------------------------------------------\n",
    "from Pegasus.api import *\n",
    "\n",
    "# --- Top Directory Setup ----------------------------------------------------------\n",
    "top_dir = Path(\".\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720e5e8",
   "metadata": {},
   "source": [
    "#### Data Acquisition and Splitting\n",
    "Image dataset has been provided in the `data/images` directory along with the annotations (class label regarding each image) in the `data/annotations` directory. Moreover the data split is done as follows:\n",
    "* 70% training\n",
    "* 10% validation\n",
    "* 20% testing\n",
    "\n",
    "<img src=\"imgs/data_split.png\" align=\"left\" style=\"width: 220px;\"/><img src=\"imgs/data_acquisition.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA AQUSITION\n",
    "imagesList = glob.glob('data/images/*.png')\n",
    "predict_images = glob.glob('data/pred_imgs/*.png')\n",
    "annotationList = glob.glob('data/annotations/*.xml')\n",
    "\n",
    "NUM_TRIALS = 2\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "#DATA SPLIT\n",
    "train_filenames,val_filenames,test_filenames, files_split_dict = split_data_filenames(imagesList)\n",
    "\n",
    "#ANNOTATIONS\n",
    "train_imgs, train_ann = create_ann_list(train_filenames)\n",
    "val_imgs, val_ann     = create_ann_list(val_filenames)\n",
    "test_imgs, test_ann   = create_ann_list(test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14f12a",
   "metadata": {},
   "source": [
    "**Note :** If you are planning to train the model properly, please be advised optimum resuslts were obtained at 25 epochs and 4 trials but since the workflow is designed to be run on CPU it may take around a minimum of **12+ hours** of training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab6dbd",
   "metadata": {},
   "source": [
    "#### Creating replica catalog and properties\n",
    "Replica catalog is crated regarding all input images and annotation files. Moreover, `dagman.retry` is set for checkpointing - if jobs fails or timeouts, Pegasus will retry the job 2 times and use the checkpoint to restart the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## PROPERTIES ###########################################################\n",
    "props = Properties()\n",
    "props[\"dagman.retry\"] = \"1\"\n",
    "props[\"pegasus.mode\"] = \"development\"\n",
    "props.write()\n",
    "\n",
    "\n",
    "###################################### REPLICA CATALOG ###########################################################\n",
    "\n",
    "rc = ReplicaCatalog()\n",
    "\n",
    "inputFiles = []\n",
    "for img in imagesList:\n",
    "    fileName = img.split(\"/\")[-1]\n",
    "    img_file = File(fileName)\n",
    "    inputFiles.append(img_file)\n",
    "    rc.add_replica(\"local\", img_file,  os.path.join(os.getcwd(),str(img)))\n",
    "\n",
    "pred_imgs = []\n",
    "for img in predict_images:\n",
    "    fileName = img.split(\"/\")[-1]\n",
    "    img_file = File(fileName)\n",
    "    pred_imgs.append(img_file)\n",
    "    rc.add_replica(\"local\", img_file,  os.path.join(os.getcwd(),str(img)))\n",
    "\n",
    "annFiles = []\n",
    "for ann in annotationList:\n",
    "    fileName = ann.split(\"/\")[-1]\n",
    "    ann_file = File(fileName)\n",
    "    annFiles.append(ann_file)\n",
    "    rc.add_replica(\"local\", ann_file,  os.path.join(os.getcwd(),str(ann)))\n",
    "\n",
    "## add checkpointing file for the hpo model job\n",
    "def create_pkl(model):\n",
    "    pkl_filename = \"hpo_study_\" + model + \".pkl\"\n",
    "    file = open(pkl_filename, 'ab')\n",
    "    pickle.dump(\"\", file, pickle.HIGHEST_PROTOCOL)\n",
    "    return pkl_filename\n",
    "\n",
    "mask_detection_pkl = create_pkl(\"mask_detection\")\n",
    "mask_detection_pkl_file = File(mask_detection_pkl)\n",
    "rc.add_replica(\"local\", mask_detection_pkl, os.path.join(os.getcwd(), mask_detection_pkl))\n",
    "\n",
    "fastRCNNP_pkl = create_pkl(\"fastRCNNP\")\n",
    "fastRCNNP_pkl_file = File(fastRCNNP_pkl)\n",
    "rc.add_replica(\"local\", fastRCNNP_pkl, os.path.join(os.getcwd(), fastRCNNP_pkl))\n",
    "\n",
    "rc.write()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72452b40",
   "metadata": {},
   "source": [
    "#### Creating the Tranformation catalog\n",
    "\n",
    "* `mask_detection_container` : the container consisting all tools and libraries in order to execute a job\n",
    "* `plot_class_distribution.py` : data exploration to visualize class distribution\n",
    "* `data_aug.py` : data augmentation of the training set images, using Gaussian noise\n",
    "* `rename_file.py` : renames the image file with ***test*** or ***val*** prefixes\n",
    "* `hpo_train.py` : hyperparameter optimization for FastRCNN model\n",
    "* `train_model.py` : training the FastRCNN model and fine-tuning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934fe2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### TRANSFORMATIONS ###########################################################\n",
    "\n",
    "# Container for all the jobs\n",
    "tc = TransformationCatalog()\n",
    "mask_detection_wf_cont = Container(\n",
    "                \"mask_detection_wf\",\n",
    "                Container.SINGULARITY,\n",
    "                image=\"docker://pegasus/mask-detection:latest\",\n",
    "                image_site=\"docker_hub\"\n",
    "            )\n",
    "\n",
    "tc.add_containers(mask_detection_wf_cont)\n",
    "\n",
    "\n",
    "dist_plot = Transformation(\n",
    "                \"dist_plot\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/plot_class_distribution.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "augment_imgs = Transformation(\n",
    "                \"augment_images\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/data_aug.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "rename_imgs = Transformation(\n",
    "                \"rename_images\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/rename_file.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "hpo_model = Transformation(\n",
    "                \"hpo_script\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/hpo_train.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "train_model = Transformation(\n",
    "                \"train_script\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/train_model.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "evaluate_model = Transformation(\n",
    "                \"evaluate_script\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/evaluate.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "predict_detection = Transformation(\n",
    "                \"predict_script\",\n",
    "                site = \"local\",\n",
    "                pfn = top_dir/\"bin/predict.py\",\n",
    "                is_stageable = True,\n",
    "                container = mask_detection_wf_cont\n",
    "            )\n",
    "\n",
    "\n",
    "tc.add_transformations(augment_imgs, dist_plot, rename_imgs, hpo_model, train_model, evaluate_model, predict_detection)\n",
    "logging.info(\"writing tc with transformations: {}, containers: {}\".format([k for k in tc.transformations], [k for k in tc.containers]))\n",
    "tc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9ed65",
   "metadata": {},
   "source": [
    "#### Creating jobs and adding it to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7669387",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### CREATE JOBS ###########################################################\n",
    "wf = Workflow(\"mask_detection_workflow\")\n",
    "\n",
    "train_preprocessed_files = create_augmented_filelist(train_filenames,2)\n",
    "distribution_plot_file = File(\"class_distribution.png\")\n",
    "val_preprocessed_files = [File(\"val_\"+ f.split(\"/\")[-1]) for f in val_filenames]\n",
    "test_preprocessed_files = [File(\"test_\"+ f.split(\"/\")[-1]) for f in test_filenames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29da655",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "Takes in all the annotations files and creates plot with distribution of the classes. It helps in detecting if class imbalance exists which affects bias regarding classification of images into threee categories as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_job = Job(dist_plot)\n",
    "distribution_plot_job.add_args(distribution_plot_file)\n",
    "distribution_plot_job.add_inputs(*train_ann, *val_ann, *test_ann)\n",
    "distribution_plot_job.add_outputs(distribution_plot_file)\n",
    "wf.add_jobs(distribution_plot_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf0e8a",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "Image augmentation is done on the training images by adding Gaussian noise to them. This helps to normalize the training dataset and maintain consistensy.\n",
    "\n",
    "<img src=\"imgs/data_augmentation.png\" style=\"width: 350px;\"/>\n",
    "\n",
    "Also, renaming of the test set images and validation set images using the prefixes ***test*** and ***val*** as input arguments respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ba109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DATA AUGMENTATION\n",
    "preprocess_train_job = Job(augment_imgs)\n",
    "preprocess_train_job.add_inputs(*train_imgs)\n",
    "preprocess_train_job.add_outputs(*train_preprocessed_files,stage_out=False)\n",
    "wf.add_jobs(preprocess_train_job)\n",
    "\n",
    "# VAL DATA-FILE RENAMING\n",
    "preprocess_val_job = Job(rename_imgs)\n",
    "preprocess_val_job.add_inputs(*val_imgs)\n",
    "preprocess_val_job.add_outputs(*val_preprocessed_files,stage_out=False)\n",
    "preprocess_val_job.add_args(\"val\")\n",
    "wf.add_jobs(preprocess_val_job)\n",
    "\n",
    "# TEST DATA-FILE RENAMING\n",
    "preprocess_test_job = Job(rename_imgs)\n",
    "preprocess_test_job.add_inputs(*test_imgs)\n",
    "preprocess_test_job.add_outputs(*test_preprocessed_files,stage_out=False)\n",
    "preprocess_test_job.add_args(\"test\")\n",
    "wf.add_jobs(preprocess_test_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dbb119",
   "metadata": {},
   "source": [
    "####  Hyperparameter optimization model\n",
    "Use Hyper-parameters optimization library `optuna` to find adequate learning\n",
    "rate, backbone model for transfer learning and more. The output is saved into a `txt` file and then used by the model training step.\n",
    "<img src=\"imgs/hpo.png\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec935a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_params = File(\"best_hpo_params.txt\")\n",
    "hpo_job = Job(hpo_model)\n",
    "hpo_job.add_args(\"--epochs\",NUM_EPOCHS, \"--trials\", NUM_TRIALS)\n",
    "hpo_job.add_inputs(*train_preprocessed_files,*train_ann,*val_preprocessed_files,*val_ann)\n",
    "hpo_job.add_outputs(hpo_params)\n",
    "hpo_job.add_checkpoint(mask_detection_pkl_file, stage_out=True)\n",
    "hpo_job.add_pegasus_profile(cores=8, memory=\"12 GB\", runtime=14400)\n",
    "wf.add_jobs(hpo_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38c9e9",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "Using the optimum hyperparameters from the last step, we train the model using the validation, testing and training image sets. The entire model is saved in `mask_detection_model.pth`, which can be used to make inferences.\n",
    "\n",
    "<img src=\"imgs/training.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92634b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = File(\"mask_detection_model.pth\")\n",
    "model_training_job = Job(train_model)\n",
    "model_training_job.add_args(hpo_params,model_file)\n",
    "model_training_job.add_inputs(hpo_params,*train_imgs,\n",
    "                              *train_preprocessed_files, *val_preprocessed_files,\n",
    "                              *test_preprocessed_files, *annFiles)\n",
    "model_training_job.add_checkpoint(fastRCNNP_pkl_file, stage_out=True)\n",
    "model_training_job.add_outputs(model_file)\n",
    "wf.add_jobs(model_training_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad8def",
   "metadata": {},
   "source": [
    "**Note :** If you are planning to train the model properly, please be advised optimum results were obtained at 25 epochs of training the model (which can be edited in `train_model.py`) but since the workflow is designed to be run on CPU it may take around a minimum of **12+ hours** of training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4276de8",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "\n",
    "Evaluate performance of the final model using test data. The confusion matrix is used regarding evalation of classification labels and plot is provided in `confusion_matrix.png`. Moreover running loss of the trained model is also provided in `evaluation.txt` file.\n",
    "\n",
    "<img src=\"imgs/cm.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1596cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_file = File(\"confusion_matrix.png\")\n",
    "evaluation_file = File(\"evaluation.txt\")\n",
    "model_evaluating_job = Job(evaluate_model)\n",
    "model_evaluating_job.add_args(model_file,evaluation_file,confusion_matrix_file)\n",
    "model_evaluating_job.add_inputs(model_file,*test_preprocessed_files, *annFiles)\n",
    "model_evaluating_job.add_outputs(evaluation_file,confusion_matrix_file)\n",
    "model_evaluating_job.add_pegasus_profile(cores=8, memory=\"12 GB\", runtime=14400)\n",
    "wf.add_jobs(model_evaluating_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7354afa",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "Predictions can be made using new images in the `/data/pred_images` with \"_pred__\" prefix. The predicted image with mask detection and predicted classes is obtained in `predicted_image.png`. Moreover, `predictions.txt` contains **confidence scores** of predicted classes along with detections, you can plot it on images using your own methods. The following figure is for reference regarding the accuracy of the predictions made by the model trained for different epochs.\n",
    "\n",
    "<img src=\"imgs/prediction.png\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15333a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image = File(\"predicted_image.png\")\n",
    "predicted_classes = File(\"predictions.txt\")\n",
    "predict_detection_job = Job(predict_detection)\n",
    "predict_detection_job.add_args(model_file,predicted_image,predicted_classes)\n",
    "predict_detection_job.add_inputs(model_file,*pred_imgs, *annFiles)\n",
    "predict_detection_job.add_outputs(predicted_image,predicted_classes)\n",
    "predict_detection_job.add_pegasus_profile(cores=8, memory=\"12 GB\", runtime=14400)\n",
    "wf.add_jobs(predict_detection_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5027a84",
   "metadata": {},
   "source": [
    "## 2. Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site **condorpool** i.e the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b160363",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(submit=True)\n",
    "except PegasusClientError as e:\n",
    "    print(e.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0257ed",
   "metadata": {},
   "source": [
    "After the workflow has been successfully planned and submitted, you can use the Python `Workflow` object in order to monitor the status of the workflow. It shows in detail the counts of jobs of each status and also the whether the job is idle or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff182c3c",
   "metadata": {},
   "source": [
    "## 3.  Launch Pilots Jobs on ACCESS resources\n",
    "\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "A pilot can run multiple user jobs - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    "\n",
    "A pilot is partitionable - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    "\n",
    "A pilot will only run jobs for the user who started it.\n",
    "\n",
    "The process of starting pilots is described in the [ACCESS Pegasus Documentation](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html)\n",
    "\n",
    "## 4.Wait for the workflow to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60dd77",
   "metadata": {},
   "source": [
    "## 5. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
