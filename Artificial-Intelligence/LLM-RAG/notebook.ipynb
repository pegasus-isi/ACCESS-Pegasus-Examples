{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e243d342",
   "metadata": {},
   "source": [
    "# Large Language Model - Retrieval Augmented Generation\n",
    "\n",
    "\n",
    "LLM RAG (Large Language Model Retrieval-Augmented Generation) is a technique that enhances large language models by incorporating information retrieval mechanisms. It involves retrieving relevant information from a database or document corpus and combining it with the original query to provide additional context. This augmented input is then processed by the large language model to generate more accurate and contextually relevant responses. LLM RAG offers benefits such as improved accuracy, access to up-to-date information, and better contextual understanding, making it useful for applications like question answering, summarization, and conversational AI.\n",
    "\n",
    "In this notebook, we will create workflow to interact with a local LLM, first asking a question with out RAG, and then the same question with RAG. The output of the workflow consists of those replies.\n",
    "\n",
    "The example was inspired by a [Medium post by Duy Huynh](https://medium.com/@vndee.huynh/build-your-own-rag-and-run-it-locally-langchain-ollama-streamlit-181d42805895) and using the following components:\n",
    "\n",
    " - [Ollama](https://python.langchain.com/v0.1/docs/integrations/llms/ollama/)\n",
    " - [Mistral LLM](https://docs.mistral.ai/)\n",
    " - [LangChain](https://python.langchain.com/v0.1/docs/integrations/llms/ollama/)\n",
    " - [Chroma Vector Storage](https://github.com/chroma-core/chroma)\n",
    "\n",
    "\n",
    "## Container\n",
    "\n",
    "An Apptainer image is used to install all the software pieces. The definition of the container can be found in `containers/llm-rag.def`. To make this example run quickly, an already built version of the image is downloaded.\n",
    "\n",
    "## Compute Job\n",
    "\n",
    "The compute job will run on a remote host, using a GPU. The job consists of two files, which both can be found in the `bin/` directory of this example:\n",
    "\n",
    "  - `wrapper.sh` - this script picks a random port, starts an ollama instance on that port, and then runs the `llm-rag.py` script\n",
    "  - `llm-rag.py` - this script uses `LangChain` to interact with the ollama instance. This is where you will find the loading of the additional data, and the prompts to the LLM.\n",
    "  \n",
    "In the script, we will first ask the LLM to describe itself:\n",
    "\n",
    "```python\n",
    "answer = model.invoke(\"Please tell me what kind of LLM you are, and describe what data you were trained on.\").content\n",
    "print(wrap(answer))\n",
    "```\n",
    "\n",
    "We will then ask out main question, but not using RAG. The quetion in this example is about the [IU Jetstream2](https://docs.jetstream-cloud.org/) resource, which is one of the target execution environments for this workflow:\n",
    "\n",
    "```python\n",
    "answer = model.invoke(f\"{instructions_regular} Please provide a summary of the GPU capabilities of the IU Jetstream2 system. Include the instance flavors, and any details about the GPUs.\").content\n",
    "print(wrap(answer))\n",
    "```\n",
    "\n",
    "The script will then load the data for RAG, consisting of the full IU Jetstream2 documentation in PDF format, and then ask the same question again. At the end, we should be able to compare the quality of the answer without and with RAG, hopefully see a big improvement in the latter answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5bc70a",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The Pegasus workflow is simple in this case. There is only one job, which takes the `llm-rag.py` script and `js2-documentation.pdf` as inputs, and produces `answers.txt` and `ollama.log` as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bcacf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# --- Import Pegasus API -----------------------------------------------------------\n",
    "from Pegasus.api import *\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# --- Main workflow class ----------------------------------------------------------\n",
    "class LLMRAGWorkflow():\n",
    "    wf = None\n",
    "    sc = None\n",
    "    tc = None\n",
    "    rc = None\n",
    "    props = None\n",
    "\n",
    "    dagfile = None\n",
    "    wf_dir = None\n",
    "    shared_scratch_dir = None\n",
    "    local_storage_dir = None\n",
    "    wf_name = \"llm-rag\"\n",
    "    \n",
    "    \n",
    "    # --- Init ---------------------------------------------------------------------\n",
    "    def __init__(self, dagfile=\"workflow.yml\"):\n",
    "        self.dagfile = dagfile\n",
    "        self.wf_dir = str(Path(\".\").resolve())\n",
    "        self.shared_scratch_dir = os.path.join(self.wf_dir, \"scratch\")\n",
    "        self.local_storage_dir = os.path.join(self.wf_dir, \"output\")\n",
    "\n",
    "    \n",
    "    # --- Write files in directory -------------------------------------------------\n",
    "    def write(self):\n",
    "        if not self.sc is None:\n",
    "            self.sc.write()\n",
    "        self.props.write()\n",
    "        self.rc.write()\n",
    "        self.tc.write()\n",
    "        \n",
    "        try:\n",
    "            self.wf.write()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    # --- Plan and Submit the workflow ----------------------------------------------\n",
    "    def plan_submit(self):\n",
    "        try:\n",
    "            self.wf.plan(submit=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "    # --- Get status of the workflow -----------------------------------------------\n",
    "    def status(self):\n",
    "        try:\n",
    "            self.wf.status(long=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "    # --- Wait for the workflow to finish -----------------------------------------------\n",
    "    def wait(self):\n",
    "        try:\n",
    "            self.wf.wait()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "    # --- Get statistics of the workflow -----------------------------------------------\n",
    "    def statistics(self):\n",
    "        try:\n",
    "            self.wf.statistics()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "\n",
    "    # --- Configuration (Pegasus Properties) ---------------------------------------\n",
    "    def create_pegasus_properties(self):\n",
    "        self.props = Properties()\n",
    "        self.props[\"pegasus.integrity.checking\"] = \"none\"\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.sc = SiteCatalog()\n",
    "\n",
    "        local = (Site(\"local\")\n",
    "                    .add_directories(\n",
    "                        Directory(Directory.SHARED_SCRATCH, self.shared_scratch_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.shared_scratch_dir, Operation.ALL)),\n",
    "                        Directory(Directory.LOCAL_STORAGE, self.local_storage_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.local_storage_dir, Operation.ALL))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        exec_site = (Site(exec_site_name)\n",
    "                        .add_condor_profile(universe=\"vanilla\")\n",
    "                        .add_pegasus_profile(\n",
    "                            style=\"condor\"\n",
    "                        )\n",
    "                    )\n",
    "        self.sc.add_sites(local, exec_site)\n",
    "        \n",
    "\n",
    "    # --- Transformation Catalog (Executables and Containers) ----------------------\n",
    "    def create_transformation_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.tc = TransformationCatalog()\n",
    "        \n",
    "        llm_rag_container = Container(\"llm_rag_container\",\n",
    "            container_type = Container.SINGULARITY,\n",
    "            image = \"https://download.pegasus.isi.edu/containers/llm-rag/llm-rag.sif\",\n",
    "            image_site = \"web\"\n",
    "        )\n",
    "        \n",
    "        # main job wrapper\n",
    "        # note how gpus and other resources are requested\n",
    "        wrapper = Transformation(\"wrapper\", \n",
    "                                 site=\"local\", \n",
    "                                 pfn=self.wf_dir+\"/bin/wrapper.sh\", \n",
    "                                 is_stageable=True, \n",
    "                                 container=llm_rag_container)\\\n",
    "                  .add_pegasus_profiles(cores=4, gpus=1, memory=\"20 GB\", diskspace=\"15 GB\")\n",
    "                  \n",
    "        self.tc.add_containers(llm_rag_container)\n",
    "        self.tc.add_transformations(wrapper)\n",
    "\n",
    "    \n",
    "    # --- Replica Catalog ----------------------------------------------------------\n",
    "    def create_replica_catalog(self):\n",
    "        self.rc = ReplicaCatalog()\n",
    "\n",
    "        # Add inference dependencies\n",
    "        self.rc.add_replica(\"local\", \"llm-rag.py\", \\\n",
    "                                     os.path.join(self.wf_dir, \"bin/llm-rag.py\"))\n",
    "        self.rc.add_replica(\"local\", \"js2-documentation.pdf\", \\\n",
    "                                     os.path.join(self.wf_dir, \"pdfs/js2-documentation.pdf\"))\n",
    "     \n",
    "\n",
    "    # --- Create Workflow ----------------------------------------------------------\n",
    "    def create_workflow(self):\n",
    "        self.wf = Workflow(self.wf_name, infer_dependencies=True)\n",
    "        \n",
    "        llm_rag_py = File(\"llm-rag.py\")\n",
    "        pdf = File(\"js2-documentation.pdf\")\n",
    "        answers_txt = File(\"answers.txt\")\n",
    "        ollama_log = File(\"ollama.log\")\n",
    "        \n",
    "        job = (Job(\"wrapper\")\n",
    "                  .add_inputs(llm_rag_py, pdf)\n",
    "                  .add_outputs(answers_txt, stage_out=True)\n",
    "                  .add_outputs(ollama_log, stage_out=True)\n",
    "              )\n",
    "        \n",
    "        self.wf.add_jobs(job)\n",
    "\n",
    "            \n",
    "dagfile = 'workflow.yml'\n",
    "\n",
    "workflow = LLMRAGWorkflow(dagfile=dagfile)\n",
    "\n",
    "print(\"Creating execution sites...\")\n",
    "workflow.create_sites_catalog(\"condorpool\")\n",
    "\n",
    "print(\"Creating workflow properties...\")\n",
    "workflow.create_pegasus_properties()\n",
    "\n",
    "print(\"Creating transformation catalog...\")\n",
    "workflow.create_transformation_catalog(\"condorpool\")\n",
    "\n",
    "print(\"Creating replica catalog...\")\n",
    "workflow.create_replica_catalog()\n",
    "\n",
    "print(\"Creating workflow dag...\")\n",
    "workflow.create_workflow()\n",
    "\n",
    "workflow.write()\n",
    "print(\"Workflow has been generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3adb46",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Plan and Submit the Workflow\n",
    "\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site **condorpool** i.e the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.plan_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76c63a",
   "metadata": {},
   "source": [
    "After the workflow has been successfully planned and submitted, you can use the Python `Workflow` object in order to monitor the status of the workflow. It shows in detail the counts of jobs of each status and also the whether the job is idle or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3380d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5a1d4",
   "metadata": {},
   "source": [
    "## Wait for the workflow to finish, and then display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af47148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/answers.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
