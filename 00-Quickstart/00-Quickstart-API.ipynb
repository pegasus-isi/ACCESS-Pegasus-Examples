{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus Quickstart\n",
    "\n",
    "Welcome to the Pegasus Quickstart tutorial notebook, which is intended for new users who want to get a quick overview of Pegasus concepts and usage. This tutorial covers:\n",
    "\n",
    " - Using the Pegasus API to generate an abstract workflow\n",
    " - Using the API to plan the abstract workflow into an executable workflow\n",
    " - Run the workflow first locally, and then have the jobs in the workflow run on remote ACCESS resources.\n",
    " \n",
    "For a quick overview of Pegasus, please see this short YouTube video:\n",
    "\n",
    "[![A 5 Minute Introduction](../images/youtube-pegasus-intro.png)](https://www.youtube.com/watch?v=MNN80OHMQUQ \"A 5 Minute Introduction\")\n",
    "\n",
    "\n",
    "## Hello World Workflow\n",
    "\n",
    "This notebook will generate a simple **Hello World** workflow illustrated below, then plan and execute the workflow. We will first run this workflow locally on the ACCESS Pegasus host, and then run on the remote ACCESS resources.\n",
    "\n",
    "Rectangles represent input/output files, and ovals represent compute jobs. The arrows represent file dependencies between each compute job. This simple workflow will execute 2 jobs, each taking in one input file and generating a single output file. The **world** job is dependant on the output of the **hello**.\n",
    "\n",
    "Each job in this workflow invokes a python executable named *hello* and *world* . These are symlinks to the same executable *pegasus-keg.py* . We use symbolic links in this case, to have the executable and the job name match. The python code is a simple code that takes in an input file; captures the hostname of the node where it is run on; and also includes the contents of the input file in it's output.\n",
    "\n",
    "![Hello World Workflow](../images/pipeline.svg)\n",
    "\n",
    "The abstract workflow description that you specify to Pegasus is portable, and usually does not contain any locations to physical input files, executables or cluster end points where jobs are executed. \n",
    "\n",
    "In this example, Pegasus will \n",
    "\n",
    "* pick up the inputs from a directory named **input** .\n",
    "* pick up the executables from a directory named **executables** .\n",
    "* place the generated outputs in a directory named **output** .\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> There is a separation between the environment where this notebook runs and where the compute job executes. This notebook runs on pegasus.access-ci.org, while the job is executed on any available HTCondor execution points. For this tutorial, a small number of execution points will be provided automatically. For larger workflows, you will learn in the `Provisioning` tutorial how to allocate additional resources using your allocations.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python API\n",
    "\n",
    "Pegasus 5.0 introduces a new Python API, which is fully documented in the [Pegasus reference guide](https://pegasus.isi.edu/documentation/reference-guide/api-reference.html). \n",
    "\n",
    "We will mainly use 3 main classes in this simple quickstart example\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "from Pegasus.api.replica_catalog import File\n",
    "from Pegasus.api.workflow import Job, Workflow\n",
    "from Pegasus.client._client import PegasusClientError\n",
    "```\n",
    "\n",
    "The `Workflow` object is used to store jobs and dependencies between each job. Typical job creation is as follows:\n",
    "\n",
    "```\n",
    "# Define job Input/Output files\n",
    "input_file = File(\"input.txt\")\n",
    "output_file1 = File(\"output1.txt\")\n",
    "output_file2 = File(\"output2.txt\")\n",
    "\n",
    "# Define job, passing in the transformation (executable) it will use\n",
    "j = Job(transformation_obj)\n",
    "\n",
    "# Specify command line arguments (if any) which will be passed to the transformation when run\n",
    "j.add_args(\"arg1\", \"arg2\", input_file, \"arg3\", output_file)\n",
    "\n",
    "# Specify input files (if any)\n",
    "j.add_inputs(input_file)\n",
    "\n",
    "# Specify output files (if any)\n",
    "j.add_outputs(output_file1, output_file2)\n",
    "\n",
    "# Add profiles to the job\n",
    "j.add_env(FOO=\"bar\")\n",
    "j.add_profiles(Namespace.PEGASUS, key=\"checkpoint.time\", value=1)\n",
    "\n",
    "# Add the job to the workflow object\n",
    "wf.add_jobs(j)\n",
    "```\n",
    "\n",
    "By default, dependencies between jobs are inferred based on input and output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# we specify directories for inputs, executables and outputs\n",
    "# - directory where to pick up the inputs from a directory.\n",
    "# - directory where the executables that the workflow uses are placed.\n",
    "# - directory where the outputs should be placed.\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "INPUT_DIR = Path(BASE_DIR /  \"input\").resolve()\n",
    "EXECUTABLES_DIR = Path(BASE_DIR / \"..\" /  \"executables\").resolve()\n",
    "OUTPUT_DIR = Path(BASE_DIR /  \"output\").resolve() \n",
    "\n",
    "# the execution site where you job to run.\n",
    "# local means the jobs run on ACCESS Pegasus itself.\n",
    "# condorpool means jobs will run on a node provisioned from an ACCESS site such as jetstream\n",
    "EXEC_SITE=\"local\"\n",
    "\n",
    "# generate a simple input file for the workflow\n",
    "with open(\"{}/f.in\".format(INPUT_DIR), \"w\") as f:\n",
    "    f.write(\"This is the contents of the input file for the hello world workflow!\")\n",
    "\n",
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"hello-world\")\n",
    "\n",
    "fin = File(\"f.in\")\n",
    "finter = File(\"f.inter\")\n",
    "fout = File(\"f.out\")\n",
    "\n",
    "job_hello = Job(\"hello\")\\\n",
    "                    .add_args(\"-T\", \"3\", \"-i\", fin, \"-o {}\".format(finter))\\\n",
    "                    .add_inputs(fin)\\\n",
    "                    .add_outputs(finter)\n",
    "\n",
    "job_world = Job(\"world\")\\\n",
    "                    .add_args(\"-T\", \"3\", \"-i\", finter, \"-o {}\".format(fout))\\\n",
    "                    .add_inputs(finter)\\\n",
    "                    .add_outputs(fout)\n",
    "\n",
    "wf.add_jobs(job_hello, job_world)    \n",
    "\n",
    "# --- Visualize the Workflow ---------------------------------------------------\n",
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the Workflow\n",
    "\n",
    "When working in Python, we can just use the reference do the `Workflow` object, you can plan, run, and monitor the workflow directly. These are wrappers around Pegasus CLI tools, and as such, the same arguments may be passed to them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(input_dirs=[INPUT_DIR], sites=[EXEC_SITE], transformations_dir=EXECUTABLES_DIR,\\\n",
    "            output_dir=OUTPUT_DIR, submit=True)\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line in the output that starts with pegasus-status, contains the command you can use to monitor the status of the workflow. We will cover this command line tool in the next couple of notbooks. The path it contains is the path to the submit directory where all of the files required to submit and monitor the workflow are stored. For now we will just continue to use the Python `Workflow` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.status(long=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the workflow to finish, and then display the results\n",
    "\n",
    "We can also just block on the workflow finishing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting the generated output of the workflow\n",
    "\n",
    "The executable that is run as part of this worklfow, is a simple python script that captures the hostname, where\n",
    "a job ran, and also includes the contents of the input file in it's output.\n",
    "\n",
    "The Hostname in the .out file indicates that the jobs ran on pegasus.access-ci.org. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/f.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Run the jobs in the workflow on ACCESS resources\n",
    "\n",
    "Since we planned the workflow for a site named **local** the jobs ran on ACCESS Pegasus host itself.\n",
    "\n",
    "Now we will plan the same workflow again and have it run on a site named **condorpool**.\n",
    "\n",
    "Note, that we are not re-defining the workflow. The workflow to Pegasus is described in a portable resource agnostic way. We are taking the same abstract workflow and now, will replan it for a different execution environment.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> There is a separation between the environment where this notebook runs and where the compute job executes. This notebook runs on pegasus.access-ci.org, while the job is executed on any available HTCondor execution points. For this tutorial, a small number of execution points will be provided automatically. For larger workflows, you will learn in the `Provisioning` tutorial how to allocate additional resources using your allocations.\n",
    "The following figure shows how workflows are defined using the Pegasus API in Jupyter, planned to an executable\n",
    "HTCondor DAGMan, and jobs flow to the remote execution sites (TestPool in this case).\n",
    "</div>\n",
    "\n",
    "<img src=\"../images/access-pegasus-jobflow.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(input_dirs=[INPUT_DIR], sites=[\"condorpool\"], transformations_dir=EXECUTABLES_DIR,\\\n",
    "            output_dir=OUTPUT_DIR, submit=True)\\\n",
    "      .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Reinspecting the output of the workflow \n",
    "\n",
    "Now lets review the output of the workflow to check where the jobs ran. The displayed hostname will have a prefix of  testpool-cpu-* . The test pool is made of nodes provisioned from the Indiana University's **Jetstream2** which is a Cloud ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/f.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
