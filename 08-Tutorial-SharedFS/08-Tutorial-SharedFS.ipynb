{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Workflow on a Single ACCESS Resource and using the Shared Filesystem \n",
    "\n",
    "**Objective:** Learn about how you can run a workflow on a single ACCESS Resource (Expanse used as an example) and use the shared filesystem on it. \n",
    "\n",
    "In the previous notebook, you learnt about how to do provisioning using the command `htcondor annex` against an ACCESS resource. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "If you don't have an allocation on Expanse, you can use the annex command against the HPC ACCESS resource you have an allocation for. \n",
    "</div>\n",
    "\n",
    "## 1. Setting up Annex against a HPC ACCESS Resource\n",
    "\n",
    "\n",
    "To launch the pilot jobs from ACCESS Pegasus submit host, use the htcondor annex create command.\n",
    "\n",
    "If you have never launched a HTCondor pilot before from ACCESS Pegasus follow the instructions here .\n",
    "\n",
    "A sample invocation against SDSC Expanse is listed below. Note: you need to do it on the command line in a terminal.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "htcondor annex create --project <project-id> --lifetime 3600   --nodes 1  $USER QUEUE@RESOURCE\n",
    "```\n",
    "\n",
    "If this is your first time doing an `annex` against a resource, you need to some one time setup. \n",
    "Please refer to \n",
    "[ACCESS Pegasus Annex documentation](https://access-ci.atlassian.net/wiki/spaces/ACCESSdocumentation/pages/564887666/HTCondor+Annex) for details.\n",
    "\n",
    "Please note the annex created should be named $USER as the ACCESS Pegasus HTCondor configuration automatically adds the annex name (same as use ACCESS Pegasus username) to the jobs as a job transform. You need to specify your project-id instead of <project-id>. And also update QUEUE and RESOURCE keywords to reflect the ACCESS resource against which you are doing the annex.\n",
    "\n",
    "Below is an invocation for doing an annex against queue named `compute` on SDSC resource `expanse` which requests 1 node (128 cores) for 60 minutes.\n",
    "\n",
    "```\n",
    "htcondor annex create --project <project-id> --lifetime 3600   --nodes 1  $USER compute@expanse\n",
    "```\n",
    "\n",
    "Please open a terminal and type the above command. Remember to update the project-id to match your project id. \n",
    "\n",
    "Sample invocation against EXPANSE is shown below.\n",
    "\n",
    "```\n",
    "htcondor annex add --project XXXX --lifetime 3600   --nodes 1  $USER compute@expanse\n",
    "This will (as the project 'XXX') request 1 nodes for 1.00 hours for an annex named 'vahi' from the queue named 'compute' on the system named \n",
    "'Expanse'.  To change the project, use --project.  To change the resources requested, use either --nodes or one or more of --cpus and --mem_mb. \n",
    " To change how long the resources are reqested for, use --lifetime (in seconds).\n",
    "This command will access the system named 'Expanse' via SSH.  To proceed, follow the prompts from that system below; to cancel, hit CTRL-C.\n",
    "Enter passphrase for key '/home/vahi/.ssh/annex': \n",
    "TOTP code for ux454545: 453580\n",
    "Thank you.\n",
    "Populating annex temporary directory... done.\n",
    "Requesting annex named 'vahi' from queue 'compute' on the system named 'Expanse'...\n",
    "    Step 8 of 8: Submitting SLURM job............    \n",
    "... requested.\n",
    "It may take some time for the system named 'Expanse' to establish the requested annex.\n",
    "To check on the status of the annex, run 'htcondor annex status vahi'.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run a workflow against the Annex\n",
    "\n",
    "We will now run the same workflow that we ran in `03-Tutorial-Software` notebook that runs each job in a container against the annex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup the Replica and Transformation Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# we specify directories for inputs, executables and outputs\n",
    "# - directory where the executables that the workflow uses are placed.\n",
    "# - directory where the outputs should be placed.\n",
    "\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "EXECUTABLES_DIR = Path(BASE_DIR / \"..\" /  \"executables\").resolve()\n",
    "OUTPUT_DIR = Path(BASE_DIR /  \"output\").resolve() \n",
    "\n",
    "# --- Replicas -----------------------------------------------------------------\n",
    "fin = File(\"f.in\").add_metadata(creator=\"vahi\")\n",
    "rc = ReplicaCatalog()\\\n",
    "    .add_replica(\"remote\", fin, \"http://download.pegasus.isi.edu/tutorial/inputs/f.in\")\\\n",
    "    .write() # written to ./replicas.yml \n",
    "\n",
    "tc = TransformationCatalog()\n",
    "        \n",
    "wf_container = Container(\"wf_container\",\n",
    "    container_type = Container.SINGULARITY,\n",
    "    image = \"http://download.pegasus.isi.edu/containers/hello-world/hello-world.sif\",\n",
    "    image_site = \"web\"\n",
    ")\n",
    "\n",
    "# For each type of job in the workflow specify a transformation\n",
    "# When you instantiate a Job() object, you specify a transformation name\n",
    "# which is a logical identifier for the executable you want to run\n",
    "# when the job is launched on a remote node. \n",
    "#\n",
    "# In this workflow, we have two transformations \"hello\" and \"world\",\n",
    "# with each mapping to the same executable that is installed in\n",
    "# the container. is_stageable parameter is set to False to indicate\n",
    "# the executable is installed in the container.\n",
    "# Note: how cpu and other resources are requested\n",
    "hello = Transformation(\"hello\", \n",
    "                         site=\"web\", \n",
    "                         pfn=\"/opt/pegasus-tutorial/pegasus-keg.py\", \n",
    "                         is_stageable=False, \n",
    "                         container=wf_container)\\\n",
    "          .add_pegasus_profiles(cores=1, memory=\"1 GB\", diskspace=\"1 GB\")\n",
    "\n",
    "world = Transformation(\"world\", \n",
    "                         site=\"web\", \n",
    "                         pfn=\"/opt/pegasus-tutorial/pegasus-keg.py\", \n",
    "                         is_stageable=False, \n",
    "                         container=wf_container)\\\n",
    "          .add_pegasus_profiles(cores=1, memory=\"1 GB\", diskspace=\"1 GB\")\n",
    "\n",
    "tc.add_containers(wf_container)\n",
    "tc.add_transformations(hello, world)\n",
    "tc.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define a site for the Annex\n",
    "\n",
    "First update the variable EXPANSE_USERNAME to match your username on EXPANSE.\n",
    "If, you are doing an annex against a resource other than EXPANSE, you need to update the full path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the variables below should be updated to refer to your\n",
    "# username on expanse. There are of the form uxXXXXX\n",
    "cluster_shared_dir = \"/expanse/lustre/scratch/ux454545/temp_project\"\n",
    "cluster_home_dir = \"/home/ux454545\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now describe the expanse site in the Site Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_SITE=\"expanse\"\n",
    "\n",
    "sc = SiteCatalog()       \n",
    "\n",
    "expanse = (Site(EXEC_SITE)\n",
    ".add_pegasus_profile(\n",
    "    style=\"condor\",\n",
    "    data_configuration=\"nonsharedfs\",\n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "exec_site_shared_scratch_dir = os.path.join(cluster_shared_dir, \"pegasuswfs/scratch\")\n",
    "exec_site_shared_storage_dir = os.path.join(cluster_home_dir, \"pegasuswfs/outputs\")\n",
    "\n",
    "expanse.add_directories(\n",
    "    Directory(Directory.SHARED_SCRATCH, exec_site_shared_scratch_dir)\n",
    "    .add_file_servers(FileServer(\"file://\" + exec_site_shared_scratch_dir, Operation.ALL)),\n",
    "    Directory(Directory.LOCAL_STORAGE, exec_site_shared_storage_dir)\n",
    "    .add_file_servers(FileServer(\"file://\" + exec_site_shared_storage_dir, Operation.ALL))\n",
    ")\n",
    "expanse.add_profiles(Namespace.ENV, LANG='C')\n",
    "expanse.add_profiles(Namespace.ENV, PYTHONUNBUFFERED='1')\n",
    "\n",
    "# exclude the ACCESS Pegasus TestPool\n",
    "# we want it to run on our annex\n",
    "expanse.add_condor_profile(requirements=\"TestPool =!= True\")\n",
    "\n",
    "sc.add_sites( expanse)\n",
    "sc.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat sites.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define and Execute the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"hello-world\")\n",
    "\n",
    "\n",
    "finter = File(\"f.inter\")\n",
    "fout = File(\"f.out\")\n",
    "\n",
    "job_hello = Job(\"hello\")\\\n",
    "                    .add_args(\"-T\", \"3\", \"-i\", fin, \"-o {}\".format(finter))\\\n",
    "                    .add_inputs(fin)\\\n",
    "                    .add_outputs(finter, stage_out=False)\n",
    "\n",
    "job_world = Job(\"world\")\\\n",
    "                    .add_args(\"-T\", \"3\", \"-i\", finter, \"-o {}\".format(fout))\\\n",
    "                    .add_inputs(finter)\\\n",
    "                    .add_outputs(fout)\n",
    "\n",
    "wf.add_jobs(job_hello, job_world)    \n",
    "\n",
    "# --- Run the Workflow ---------------------------------------------------\n",
    "# we have omitted the transformations_dir argument as we have specified a Transformation \n",
    "# Catalog to specify locations of executables .  \n",
    "try:\n",
    "    wf.write()\n",
    "    wf.plan(sites=[EXEC_SITE], output_site=[EXEC_SITE], submit=True)\\\n",
    "      .wait()      \n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting the generated output of the workflow\n",
    "\n",
    "Now lets review the output of the workflow to check where the jobs ran. The displayed hostname will have a prefix of  testpool-cpu-* . The test pool is made of nodes provisioned from the Indiana University's **Jetstream2** which is a Cloud ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat output/f.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
