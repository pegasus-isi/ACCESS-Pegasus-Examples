{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Complete Workflow\n",
    "\n",
    "**Objective:** Familiarize users with the Pegasus workflow structure using an end-to-end LLM-RAG book summarization example.\n",
    "\n",
    "Welcome to the first Pegasus tutorial notebook, which is intended for new users who want to get a quick overview of running a Pegasus workflow. \n",
    "\n",
    "In this tutorial, a full workflow is provided. In later tutorials, we will learn how to use the API, the provided debugging/statistics tools, and how to provision resources for the workflow to execute on. The outline of those tutorials is:\n",
    "\n",
    " - 01 - Running a Complete Workflow (this one)\n",
    " - 02 - API\n",
    " - 03 - Catalogs\n",
    " - 04 - Debugging / Statistics\n",
    " - 05 - Provisioning\n",
    "\n",
    "To get started, just step through the following steps.\n",
    "\n",
    "## Defining the Workflow\n",
    "\n",
    "Pegasus workflows are created using an API, making it easy to build, manage, and run workflows in a flexible and scalable way. While it might feel unnecessary for small workflows, this approach works well for creating workflows dynamically based on data, parameters, or triggers, which is essential for automating tasks and handling large projects.\n",
    "\n",
    "The following example is organized as a Python class. While this isn’t strictly necessary, it helps keep the different parts of the workflow well-structured.\n",
    "\n",
    "Pegasus workflows are portable, meaning you can execute the same workflow on different infrastructures at different times. To enable this portability, Pegasus uses an abstract workflow model and relies on \"catalogs\" to describe the execution environment, software, and input data.  The Abstract Workflow description that you specify to Pegasus is portable, and usually does not contain\n",
    "\n",
    "* any locations to physical input files, \n",
    "* locations to executables referred to by the job\n",
    "* cluster end points where jobs are executed.\n",
    "\n",
    "Pegasus uses three information catalogs during the planning process.\n",
    "\n",
    "<img src=\"../03-Tutorial-Catalogs/images/catalogs.png\"/>\n",
    "\n",
    "These catalogs will be explained in detail in a later chapter.Also, the Pegasus documentation provides more details about catalogs [here](https://pegasus.isi.edu/documentation/user-guide/creating-workflows.html#catalogs)\n",
    "\n",
    "\n",
    "For now, focus on the workflow definition within the `create_workflow(self)` method.\n",
    "\n",
    "This workflow is simple: it includes just one job. The job takes a Gutenberg book in plain text format, processes it using an LLM (Large Language Model) with Retrieval-Augmented Generation (RAG), and produces two output files: an answer file and a log file.\n",
    "\n",
    "The script, `llm-rag.py`, contains the code for this job. Inside the script, you’ll find prompts for the LLM, such as:\n",
    "\n",
    " - *Please tell me what kind of LLM you are, and describe what data you were trained on.*\n",
    " - *Please provide a one paragraph summary of the book.*\n",
    " - *Who is the protagonist in the book?*\n",
    " - *Who is the antagonist in the book?*\n",
    " - *What time period is the book set in?*\n",
    " \n",
    "The goal is to create a workflow that runs this single job in a container with the LLM model, uses a GPU for processing, and returns the answers.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> There is a separation between the environment where this notebook runs and where the compute job executes. This notebook runs on pegasus.access-ci.org, while the job is executed on any available HTCondor execution points. For this tutorial, a small number of execution points will be provided automatically. For larger workflows, you will learn in the `Provisioning` tutorial how to allocate additional resources using your allocations.\n",
    "The following figure shows how workflows are defined using the Pegasus API in Jupyter, planned to an executable\n",
    "HTCondor DAGMan, and jobs flow to the remote execution sites (TestPool in this case).\n",
    "</div>\n",
    "\n",
    "<img src=\"../images/access-pegasus-jobflow.png\"/>\n",
    "\n",
    "The `Workflow` object is used to store jobs and dependencies between each job. Typical job creation is as follows:\n",
    "\n",
    "```\n",
    "        # input files\n",
    "        llm_rag_py = File(\"llm-rag.py\")\n",
    "        book = File(\"Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt\")\n",
    "        \n",
    "        # output files - these will be generated by the job\n",
    "        answers_txt = File(f\"{book}-answers.txt\")\n",
    "        ollama_log = File(f\"{book}-ollama.log\")\n",
    "        \n",
    "        # define the job\n",
    "        job = Job(\"llm-wrapper\")\n",
    "        \n",
    "        # specify command line arguments (if any)\n",
    "        job.add_args(book)\n",
    "        \n",
    "        # associate the input files\n",
    "        job.add_inputs(llm_rag_py, book)\n",
    "        \n",
    "        # associate the output files\n",
    "        job.add_outputs(answers_txt, ollama_log)\n",
    "\n",
    "        # add the job to the workflow\n",
    "        self.wf.add_jobs(job)\n",
    "```\n",
    "\n",
    "By default, dependencies between jobs are inferred based on input and output files. \n",
    "\n",
    "Let's run a full workflow. This example has a bunch of helper methods to aid with logging and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from Pegasus.api import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class LLMRAGBooks:\n",
    "    \n",
    "    BASE_DIR = Path(\".\").resolve()\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.props = Properties()\n",
    "\n",
    "        self.wf = Workflow(\"llm-rag-books\")\n",
    "        self.tc = TransformationCatalog()\n",
    "        self.sc = SiteCatalog()\n",
    "        self.rc = ReplicaCatalog()\n",
    "\n",
    "        self.wf.add_transformation_catalog(self.tc)\n",
    "        self.wf.add_site_catalog(self.sc)\n",
    "        self.wf.add_replica_catalog(self.rc)\n",
    "        \n",
    "        self.wf_dir = str(Path(\".\").resolve())\n",
    "        self.shared_scratch_dir = os.path.join(self.wf_dir, \"scratch\")\n",
    "        self.local_storage_dir = os.path.join(self.wf_dir, \"output\")\n",
    "    \n",
    "    \n",
    "    # --- Write files in directory -------------------------------------------------\n",
    "    def write(self):\n",
    "        self.props.write()\n",
    "        self.sc.write()\n",
    "        self.rc.write()\n",
    "        self.tc.write()\n",
    "        \n",
    "        try:\n",
    "            self.wf.write()\n",
    "            # also graph the workflow\n",
    "            self.wf.graph(include_files=True,  label=\"xform\", output=\"graph.png\")\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    # --- Plan and Submit the workflow ----------------------------------------------\n",
    "    def plan_submit(self):\n",
    "        try:\n",
    "            self.wf.plan(submit=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "    # --- Get status of the workflow -----------------------------------------------\n",
    "    def status(self):\n",
    "        try:\n",
    "            self.wf.status(long=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "\n",
    "            \n",
    "    # --- Wait for the workflow to finish -----------------------------------------------\n",
    "    def wait(self):\n",
    "        try:\n",
    "            self.wf.wait()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "    # --- Get statistics of the workflow -----------------------------------------------\n",
    "    def statistics(self):\n",
    "        try:\n",
    "            self.wf.statistics()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "    # --- Configuration (Pegasus Properties) ---------------------------------------\n",
    "    def create_pegasus_properties(self):\n",
    "        \n",
    "        # Help Pegasus developers by sharing performance data (optional)\n",
    "        self.props[\"pegasus.monitord.encoding\"] = \"json\"\n",
    "        self.props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "\n",
    "        # nicer looking submit dirs\n",
    "        self.props[\"pegasus.dir.useTimestamp\"] = \"true\"\n",
    "\n",
    "        \n",
    "    # --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.sc = SiteCatalog()\n",
    "\n",
    "        local = (Site(\"local\")\n",
    "                    .add_directories(\n",
    "                        Directory(Directory.SHARED_SCRATCH, self.shared_scratch_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.shared_scratch_dir, Operation.ALL)),\n",
    "                        Directory(Directory.LOCAL_STORAGE, self.local_storage_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.local_storage_dir, Operation.ALL))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        condorpool = (Site(exec_site_name)\n",
    "                        .add_condor_profile(universe=\"container\")\n",
    "                        .add_pegasus_profile(\n",
    "                            style=\"condor\"\n",
    "                        )\n",
    "                    )\n",
    "        condorpool.add_profiles(Namespace.ENV, LANG='C')\n",
    "        condorpool.add_profiles(Namespace.ENV, PYTHONUNBUFFERED='1')\n",
    "        \n",
    "        # exclude the ACCESS Pegasus TestPool \n",
    "        #condorpool.add_condor_profile(requirements=\"TestPool =!= True\")\n",
    "\n",
    "        # If you want to run on OSG, please specify your OSG ProjectName. For testing, feel\n",
    "        # free to use the USC_Deelman project (the PI of the Pegasus project).For\n",
    "        # production work, please use your own project.\n",
    "        #condorpool.add_profiles(Namespace.CONDOR, key=\"+ProjectName\", value=\"\\\"USC_Deelman\\\"\")\n",
    "        \n",
    "        self.sc.add_sites(local, condorpool)\n",
    "        \n",
    "\n",
    "    # --- Transformation Catalog (Executables and Containers) ----------------------\n",
    "    def create_transformation_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.tc = TransformationCatalog()\n",
    "        \n",
    "        llm_rag_container = Container(\"llm_rag_container\",\n",
    "            container_type = Container.SINGULARITY,\n",
    "            image = \"https://usgs2.osn.mghpcc.org/pegasus-tutorials/containers/llm-rag-v2.sif\",\n",
    "            image_site = \"web\"\n",
    "        )\n",
    "        \n",
    "        # main job wrapper\n",
    "        # note how gpus and other resources are requested\n",
    "        wrapper = Transformation(\"llm-wrapper\", \n",
    "                                 site=\"local\", \n",
    "                                 pfn=self.wf_dir+\"/bin/llm-wrapper.sh\", \n",
    "                                 is_stageable=True, \n",
    "                                 container=llm_rag_container)\\\n",
    "                  .add_pegasus_profiles(cores=1, gpus=1, memory=\"10 GB\", diskspace=\"15 GB\")\\\n",
    "                  .add_profiles(Namespace.CONDOR, key=\"require_gpus\", value=\"Capability >= 8.0\")\n",
    "\n",
    "        \n",
    "        self.tc.add_containers(llm_rag_container)\n",
    "        self.tc.add_transformations(wrapper)\n",
    "\n",
    "    \n",
    "    # --- Replica Catalog ----------------------------------------------------------\n",
    "    def create_replica_catalog(self):\n",
    "        self.rc = ReplicaCatalog()\n",
    "\n",
    "        # Add inference dependencies\n",
    "        self.rc.add_replica(\"local\", \"llm-rag.py\", \\\n",
    "                                     os.path.join(self.wf_dir, \"bin/llm-rag.py\"))\n",
    "        self.rc.add_replica(\"local\", \"Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt\", \\\n",
    "                                     os.path.join(self.wf_dir, \"inputs/Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt\"))\n",
    "     \n",
    "\n",
    "    # --- Create Workflow ----------------------------------------------------------\n",
    "    def create_workflow(self):\n",
    "        self.wf = Workflow(name=\"llm-rag-books\", infer_dependencies=True)\n",
    "        \n",
    "        # existing files - already listed in the replica catalog\n",
    "        llm_rag_py = File(\"llm-rag.py\")\n",
    "        book = File(\"Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt\")\n",
    "        \n",
    "        # these will be generated by the workflow\n",
    "        answers_txt = File(f\"{book}-answers.txt\")\n",
    "        ollama_log = File(f\"{book}-ollama.log\")\n",
    "        \n",
    "        job = (Job(\"llm-wrapper\")\n",
    "                  .add_args(book)\n",
    "                  .add_inputs(llm_rag_py, book)\n",
    "                  .add_outputs(answers_txt, stage_out=True)\n",
    "                  .add_outputs(ollama_log, stage_out=True)\n",
    "              )\n",
    "        \n",
    "        self.wf.add_jobs(job)\n",
    "\n",
    "            \n",
    "workflow = LLMRAGBooks()\n",
    "\n",
    "print(\"Creating execution sites...\")\n",
    "workflow.create_sites_catalog(\"condorpool\")\n",
    "\n",
    "print(\"Creating workflow properties...\")\n",
    "workflow.create_pegasus_properties()\n",
    "\n",
    "print(\"Creating transformation catalog...\")\n",
    "workflow.create_transformation_catalog(\"condorpool\")\n",
    "\n",
    "print(\"Creating replica catalog...\")\n",
    "workflow.create_replica_catalog()\n",
    "\n",
    "print(\"Creating workflow dag...\")\n",
    "workflow.create_workflow()\n",
    "\n",
    "workflow.write()\n",
    "print(\"Workflow has been generated!\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning the Workflow\n",
    "\n",
    "The next step is to plan the workflow, which means Pegasus takes an abstract workflow - a high-level representation of tasks, dependencies, and required resources - and transforms it into an executable workflow tailored for the target execution environment.\n",
    "\n",
    "We will also save a step here, and submit the planned workflow in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.plan_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract workflow is written out to an image for easy inspection. In this case, the workflow is really simple (only one job), but we can see that the expected input and outputs look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the planning phase includes example command-line commands for monitoring and interacting with the workflow. While these commands are always available, you can also use the Python `workflow` object directly within the notebook. This object provides detailed insights into the workflow's status, including the number of jobs in each state (e.g., idle, running, or completed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the workflow to finish, and then display the results\n",
    "\n",
    "We can also just block on the workflow finishing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Results\n",
    "\n",
    "Once the workflow has finished, we can look at the answers file for our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output/Alices_Adventures_in_Wonderland_by_Lewis_Carroll.txt-answers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "To continue exploring Pegasus, the next tutorial notebook will provide an overview how to interact with the Pegasus API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
